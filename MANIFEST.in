## `MANIFEST.in` (sdist includes match wheel package-data)

```ini
# MANIFEST.in — include non-Python runtime assets in sdist
recursive-include src/spectramind/configs *.yml *.yaml
recursive-include src/spectramind/schemas *.json
recursive-include src/spectramind/assets/diagrams *.mmd *.svg *.png

# Common top-level docs
include README.md LICENSE VERSION

# Exclude obvious junk
global-exclude *.py[cod] __pycache__ *.so *.dylib *.dll *.bak *.tmp
```

Why: matches `[tool.setuptools.package-data]` in your `pyproject.toml`, so sdists and wheels ship identical config/schema/asset payloads.

---

## `src/spectramind/__init__.py` (robust version export)

```python
# src/spectramind/__init__.py
from __future__ import annotations

from importlib import metadata

__all__ = ["__version__"]

def _detect_version() -> str:
    try:
        # Package name as declared in pyproject
        return metadata.version("spectramind-v50")
    except metadata.PackageNotFoundError:
        # Fallback for dev/editable installs before the package is built
        return "0.0.0"

__version__ = _detect_version()
```

---

## `src/spectramind/__main__.py` (keeps `python -m spectramind` in sync with console script)

```python
# src/spectramind/__main__.py
from __future__ import annotations

from .cli import app

def main() -> None:
    app()

if __name__ == "__main__":
    main()
```

---

## `src/spectramind/cli.py` (thin, typed Typer app; Hydra-aware; friendly errors)

```python
# src/spectramind/cli.py
from __future__ import annotations

import json
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

import typer
from hydra import compose, initialize
from omegaconf import OmegaConf
from rich.console import Console
from rich.traceback import install as rich_traceback

from . import __version__

app = typer.Typer(
    name="spectramind",
    no_args_is_help=True,
    help="SpectraMind V50 — NeurIPS 2025 Ariel Data Challenge pipeline",
)

console = Console()
rich_traceback(show_locals=False, width=120, word_wrap=True)

# -------------------------- Utilities --------------------------

@dataclass
class RunResult:
    ok: bool
    info: Dict[str, Any]

def _cfg_root() -> Path:
    # packaged configs live in: src/spectramind/configs
    return Path(__file__).resolve().parent / "configs"

def _load_cfg(config_name: str, overrides: List[str]) -> Dict[str, Any]:
    cfg_dir = _cfg_root()
    if not cfg_dir.exists():
        raise RuntimeError(f"Could not locate configs at {cfg_dir}")
    with initialize(version_base=None, config_path=str(cfg_dir)):
        cfg = compose(config_name=config_name, overrides=overrides)
    return OmegaConf.to_container(cfg, resolve=True)  # type: ignore[no-any-return]

def _print_manifest(title: str, cfg: Dict[str, Any]) -> None:
    console.rule(f"[bold cyan]{title}")
    console.print_json(json.dumps(cfg, indent=2, sort_keys=True))

def _fail(msg: str, code: int = 1) -> None:
    console.print(f"[bold red]ERROR:[/bold red] {msg}")
    raise typer.Exit(code)

# -------------------------- Delegates (wire real implementations later) --------------------------

def _run_calibrate(cfg: Dict[str, Any]) -> RunResult:
    # from .pipeline.calibrate import run as run_cal
    # return run_cal(cfg)
    out_dir = Path(cfg.get("paths", {}).get("calib_dir", "outputs/calib"))
    out_dir.mkdir(parents=True, exist_ok=True)
    return RunResult(ok=True, info={"calib_dir": str(out_dir)})

def _run_train(cfg: Dict[str, Any]) -> RunResult:
    # from .pipeline.train import run as run_train
    # return run_train(cfg)
    ckpt = Path(cfg.get("paths", {}).get("checkpoint", "artifacts/model.ckpt"))
    ckpt.parent.mkdir(parents=True, exist_ok=True)
    ckpt.write_bytes(b"stub-checkpoint")
    return RunResult(ok=True, info={"checkpoint": str(ckpt)})

def _run_predict(cfg: Dict[str, Any]) -> RunResult:
    # from .pipeline.predict import run as run_pred
    # return run_pred(cfg)
    pred = Path(cfg.get("paths", {}).get("predictions", "predictions/preds.csv"))
    pred.parent.mkdir(parents=True, exist_ok=True)
    if not pred.exists():
        header = "sample_id," + ",".join([f"mu_{i:03d}" for i in range(283)]) + "," + ",".join(
            [f"sigma_{i:03d}" for i in range(283)]
        )
        pred.write_text(header + "\n")
    return RunResult(ok=True, info={"predictions": str(pred)})

def _run_submit(cfg: Dict[str, Any]) -> RunResult:
    # from .pipeline.submit import run as run_submit
    # return run_submit(cfg)
    bundle = Path(cfg.get("paths", {}).get("bundle", "artifacts/submission.zip"))
    bundle.parent.mkdir(parents=True, exist_ok=True)
    return RunResult(ok=True, info={"bundle": str(bundle)})

# -------------------------- Commands --------------------------

@app.callback()
def _version_callback(
    version: Optional[bool] = typer.Option(
        None, "--version", "-V", help="Show version and exit.", is_eager=True
    )
) -> None:
    if version:
        console.print(f"spectramind-v50 {__version__}")
        raise typer.Exit(0)

@app.command(help="Calibrate sensors/data (ADC→CDS→dark/flat→trace→phase).")
def calibrate(
    config_name: str = typer.Option("train", help="Hydra config name to load."),
    override: List[str] = typer.Argument(default_factory=list, help="Hydra overrides, e.g., +env=local"),
) -> None:
    try:
        cfg = _load_cfg(config_name, override)
        _print_manifest("Calibration Config", cfg)
        res = _run_calibrate(cfg)
        if not res.ok:
            _fail("Calibration failed.")
        console.print(f"[green]✓ Calibrated[/green] → {res.info}")
    except Exception as e:
        _fail(str(e))

@app.command(help="Train fusion model (FGS1+AIRS encoders → heteroscedastic decoder).")
def train(
    config_name: str = typer.Option("train", help="Hydra config name to load."),
    override: List[str] = typer.Argument(default_factory=list),
) -> None:
    try:
        cfg = _load_cfg(config_name, override)
        _print_manifest("Training Config", cfg)
        res = _run_train(cfg)
        if not res.ok:
            _fail("Training failed.")
        console.print(f"[green]✓ Trained[/green] → {res.info}")
    except Exception as e:
        _fail(str(e))

@app.command(help="Predict μ/σ across 283 bins.")
def predict(
    config_name: str = typer.Option("predict", help="Hydra config name to load."),
    override: List[str] = typer.Argument(default_factory=list),
) -> None:
    try:
        cfg = _load_cfg(config_name, override)
        _print_manifest("Prediction Config", cfg)
        res = _run_predict(cfg)
        if not res.ok:
            _fail("Prediction failed.")
        console.print(f"[green]✓ Predicted[/green] → {res.info}")
    except Exception as e:
        _fail(str(e))

@app.command(help="Package submission artifacts (zip/csv/schema).")
def submit(
    config_name: str = typer.Option("submit", help="Hydra config name to load."),
    override: List[str] = typer.Argument(default_factory=list),
) -> None:
    try:
        cfg = _load_cfg(config_name, override)
        _print_manifest("Submit Config", cfg)
        res = _run_submit(cfg)
        if not res.ok:
            _fail("Submit failed.")
        console.print(f"[green]✓ Submit[/green] → {res.info}")
    except Exception as e:
        _fail(str(e))
```

### Minimal Hydra configs (so CLI runs out-of-box)

```
# src/spectramind/configs/train.yaml
paths:
  calib_dir: outputs/calib
  checkpoint: artifacts/model.ckpt
training:
  seed: 42
  epochs: 1
  batch_size: 8
  device: "cuda:0"

# src/spectramind/configs/predict.yaml
paths:
  predictions: predictions/preds.csv
  checkpoint: artifacts/model.ckpt
inference:
  device: "cuda:0"
  batch_size: 64

# src/spectramind/configs/submit.yaml
paths:
  bundle: artifacts/submission.zip
  predictions_dir: predictions
schema:
  json_schema: schemas/submission.schema.json
```

---

## `src/spectramind/schemas/submission.schema.json`

```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "Ariel Challenge Submission",
  "type": "object",
  "properties": {
    "id": { "type": "string" }
  },
  "patternProperties": {
    "^mu_\\d{3}$": { "type": "number" },
    "^sigma_\\d{3}$": { "type": "number" }
  },
  "required": ["id"],
  "additionalProperties": false
}
```

> If the challenge requires a different ID column name, adjust `"id"` accordingly.

---

## `scripts/package_submission.sh` (safe, loud packager)

```bash
#!/usr/bin/env bash
# scripts/package_submission.sh
set -Eeuo pipefail

ARTIFACTS_DIR="${ARTIFACTS_DIR:-artifacts}"
PRED_DIR="${PRED_DIR:-predictions}"
OUT_ZIP="${OUT_ZIP:-${ARTIFACTS_DIR}/submission.zip}"

mkdir -p "${ARTIFACTS_DIR}"

# Collect candidate files (CSV/JSON/Parquet)
mapfile -t FILES < <(find "${PRED_DIR}" -type f \( -name '*.csv' -o -name '*.json' -o -name '*.parquet' \) | sort)

if [[ ${#FILES[@]} -eq 0 ]]; then
  echo "::error::No predictions found under ${PRED_DIR}"
  exit 1
fi

echo ">> Packing ${#FILES[@]} files → ${OUT_ZIP}"
rm -f "${OUT_ZIP}"
zip -j -r "${OUT_ZIP}" "${FILES[@]}"

if [[ ! -f "${OUT_ZIP}" ]]; then
  echo "::error::Failed to create ${OUT_ZIP}"
  exit 1
fi
echo "::notice::Created ${OUT_ZIP}"
```

> `chmod +x scripts/package_submission.sh`

---

### Smoke test (local)

```bash
# After installing your local env (make dev)
spectramind calibrate
spectramind train
spectramind predict
scripts/package_submission.sh
```

This gives you:

* `artifacts/model.ckpt` (stub for now)
* `predictions/preds.csv` (format scaffold)
* `artifacts/submission.zip` (ready for validation/upload)

name: kaggle-notebook-ci

on:
  push:
    branches: [main, develop]
    paths:
      - "notebooks/**"
      - "kaggle/**"
      - "src/**"
      - "configs/**"
      - "requirements-kaggle.txt"
      - ".github/workflows/kaggle_notebook_ci.yml"
  pull_request:
    branches: [main, develop]
    paths:
      - "notebooks/**"
      - "kaggle/**"
      - "src/**"
      - "configs/**"
      - "requirements-kaggle.txt"
      - ".github/workflows/kaggle_notebook_ci.yml"
  workflow_dispatch:

permissions:
  contents: read

concurrency:
  group: kaggle-notebook-ci-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.11"
  # Default competition; notebooks may override via parameters/metadata
  KAGGLE_COMP: "ariel-data-challenge-2025"
  # Encourage Kaggle-like behavior (no internet, deterministic)
  KAGGLE_OFFLINE: "1"
  PYTHONHASHSEED: "0"

jobs:
  notebook-lint:
    name: Lint notebooks (nbqa ruff/black) + output hygiene
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v5

      - uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"
          cache-dependency-path: |
            requirements-kaggle.txt
            requirements.txt
            requirements-dev.txt
            pyproject.toml

      - name: Install lint deps
        run: |
          python -m pip install --upgrade pip
          pip install nbqa ruff black nbstripout

      - name: nbqa ruff
        run: |
          nbqa ruff notebooks --output-format=github

      - name: nbqa black --check
        run: |
          nbqa black --check notebooks

      - name: Enforce stripped outputs (optional; warn-only)
        continue-on-error: true
        run: |
          set -euo pipefail
          bad=0
          while IFS= read -r -d '' nb; do
            if jq -e '.. | .outputs? // empty | length > 0' "$nb" >/dev/null; then
              echo "::warning file=$nb::Notebook contains cell outputs; consider stripping."
              bad=1
            fi
          done < <(git ls-files 'notebooks/**/*.ipynb' -z || true)
          exit 0  # warn-only; flip to 'exit $bad' to enforce

  local-validate:
    name: Execute notebooks (offline-like; CPU)
    runs-on: ubuntu-latest
    timeout-minutes: 35
    needs: notebook-lint
    steps:
      - name: Checkout
        uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"
          cache-dependency-path: |
            requirements-kaggle.txt
            requirements.txt
            requirements-dev.txt
            pyproject.toml
            setup.cfg
            setup.py

      - name: Install minimal Kaggle-friendly deps
        run: |
          python -m pip install --upgrade pip wheel
          if [ -f requirements-kaggle.txt ]; then pip install -r requirements-kaggle.txt; fi
          pip install jupyter nbconvert nbclient nbformat papermill ipykernel jq
          pip install -q numpy pandas matplotlib || true
          # install package (imports) without heavy extras
          if [ -f pyproject.toml ] || [ -f setup.py ]; then pip install -e .; fi

      - name: Select notebooks for validation
        id: select
        shell: bash
        run: |
          set -euo pipefail
          mapfile -t NB < <(git ls-files 'notebooks/**.ipynb' | grep -E '(00_|01_|10_|smoke|quick|sample|mini).*\.ipynb$' || true)
          if [ "${#NB[@]}" -eq 0 ]; then
            mapfile -t NB < <(git ls-files 'notebooks/**.ipynb' | head -n 1 || true)
          fi
          printf 'files=%s\n' "$(printf '%s ' "${NB[@]}")" >> "$GITHUB_OUTPUT"

      - name: Execute notebooks (copy; HTML preview)
        if: steps.select.outputs.files != ''
        shell: bash
        env:
          KAGGLE_OFFLINE: ${{ env.KAGGLE_OFFLINE }}
          PYTHONWARNINGS: "ignore"
        run: |
          set -euo pipefail
          mkdir -p .ci_run/notebooks_html
          IFS=' ' read -r -a NB <<< "${{ steps.select.outputs.files }}"
          for src in "${NB[@]}"; do
            base="$(basename "$src")"
            out=".ci_run/${base}"
            cp "$src" "$out"
            echo "::group::Execute $src"
            jupyter nbconvert --execute --to notebook --inplace "$out" --ExecutePreprocessor.timeout=900
            jupyter nbconvert --to html "$out" --output ".ci_run/notebooks_html/${base%.ipynb}.html"
            echo "::endgroup::"
          done

      - name: Upload executed notebooks & HTML previews
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: notebooks-executed
          path: |
            .ci_run/*.ipynb
            .ci_run/notebooks_html/*.html
          retention-days: 7

  validate-kaggle-metadata:
    name: Validate Kaggle kernel & dataset metadata
    runs-on: ubuntu-latest
    needs: local-validate
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v5

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Validate kernel-metadata.json (if present)
        shell: bash
        run: |
          set -euo pipefail
          files=$(git ls-files 'kaggle/kernels/**/kernel-metadata.json' || true)
          [ -z "$files" ] && { echo "No kernel metadata files found."; exit 0; }
          for f in $files; do
            echo "Checking: $f"
            jq -e '.id and .title and (.is_private|type=="boolean") and (.kernel_type|IN("notebook","script")) and .language' "$f" >/dev/null
            comp_env="${{ env.KAGGLE_COMP }}"
            if [ -n "$comp_env" ]; then
              if ! jq -e --arg c "$comp_env" '.competitionSources? // [] | index($c) != null' "$f" >/dev/null; then
                echo "::notice file=$f::No competitionSources entry for '$comp_env' (ok if intentional)."
              fi
            fi
          done

      - name: Validate dataset-metadata.json (if present)
        shell: bash
        run: |
          set -euo pipefail
          files=$(git ls-files 'kaggle/datasets/**/dataset-metadata.json' || true)
          [ -z "$files" ] && { echo "No dataset metadata files found."; exit 0; }
          for f in $files; do
            echo "Checking: $f"
            jq -e '.id and .title and (.licenses|length>=1)' "$f" >/dev/null
          done

  push-to-kaggle:
    name: Push kernels to Kaggle (guarded)
    runs-on: ubuntu-latest
    needs: [local-validate, validate-kaggle-metadata]
    timeout-minutes: 20
    if: ${{ (github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch') && secrets.KAGGLE_USERNAME && secrets.KAGGLE_KEY }}
    steps:
      - name: Checkout
        uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"
          cache-dependency-path: |
            requirements-kaggle.txt

      - name: Install Kaggle CLI
        run: |
          python -m pip install --upgrade pip
          pip install kaggle jq

      - name: Configure Kaggle credentials
        run: |
          mkdir -p ~/.kaggle
          printf '{"username":"%s","key":"%s"}\n' "${{ secrets.KAGGLE_USERNAME }}" "${{ secrets.KAGGLE_KEY }}" > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json
          kaggle --version

      - name: Discover kernel workspaces
        id: kernels
        shell: bash
        run: |
          set -euo pipefail
          mapfile -t KDIRS < <(git ls-files 'kaggle/kernels/**/kernel-metadata.json' | xargs -n1 dirname 2>/dev/null || true)
          if [ "${#KDIRS[@]}" -eq 0 ]; then
            echo "skip=true" >> "$GITHUB_OUTPUT"
          else
            printf 'dirs=%s\n' "$(printf '%s ' "${KDIRS[@]}")" >> "$GITHUB_OUTPUT"
          fi

      - name: Push kernels
        if: steps.kernels.outputs.skip != 'true'
        shell: bash
        env:
          KAGGLE_COMP: ${{ env.KAGGLE_COMP }}
        run: |
          set -euo pipefail
          IFS=' ' read -r -a KDIRS <<< "${{ steps.kernels.outputs.dirs }}"
          for d in "${KDIRS[@]}"; do
            echo "::group::kaggle kernels push -p $d"
            kaggle kernels push -p "$d" | tee "kaggle_push_$(basename "$d").log"
            echo "::endgroup::"
          done

      - name: Upload Kaggle push logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: kaggle-push-logs
          path: kaggle_push_*.log
          retention-days: 7

  dataset-sync:
    name: Sync datasets to Kaggle (optional)
    runs-on: ubuntu-latest
    needs: [local-validate, validate-kaggle-metadata]
    timeout-minutes: 20
    if: ${{ (github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch') && secrets.KAGGLE_USERNAME && secrets.KAGGLE_KEY && hashFiles('kaggle/datasets/**/dataset-metadata.json') != '' }}
    steps:
      - uses: actions/checkout@v5

      - uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Kaggle CLI
        run: |
          python -m pip install --upgrade pip
          pip install kaggle

      - name: Version datasets
        shell: bash
        run: |
          set -euo pipefail
          for meta in $(git ls-files 'kaggle/datasets/**/dataset-metadata.json'); do
            d=$(dirname "$meta")
            echo "::group::kaggle datasets version -p $d"
            kaggle datasets version -p "$d" -m "CI sync: ${GITHUB_SHA::7}" -r zip | tee "kaggle_dataset_$(basename "$d").log"
            echo "::endgroup::"
          done

      - name: Upload dataset logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: kaggle-dataset-logs
          path: kaggle_dataset_*.log
          retention-days: 7

name: kaggle-notebook-ci

on:
  push:
    branches: [ main, develop ]
    paths:
      - "notebooks/**"
      - "kaggle/**"
      - "src/**"
      - "configs/**"
      - "requirements-kaggle.txt"
      - ".github/workflows/kaggle_notebook_ci.yml"
  pull_request:
    branches: [ main, develop ]
    paths:
      - "notebooks/**"
      - "kaggle/**"
      - "src/**"
      - "configs/**"
      - "requirements-kaggle.txt"
      - ".github/workflows/kaggle_notebook_ci.yml"
  workflow_dispatch:
    inputs:
      include_heavy:
        description: 'Also run notebooks tagged as heavy (true/false)'
        required: false
        default: 'false'

permissions:
  contents: read

concurrency:
  group: kaggle-notebook-ci-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.11"
  KAGGLE_COMP: "ariel-data-challenge-2025"   # notebooks may override
  KAGGLE_OFFLINE: "1"                        # emulate Kaggle's no-internet
  PIP_DISABLE_PIP_VERSION_CHECK: "1"
  PIP_NO_PYTHON_VERSION_WARNING: "1"
  PYTHONHASHSEED: "0"
  MPLBACKEND: "Agg"
  PYTHONWARNINGS: "ignore:::pkg_resources,ignore::UserWarning"
  SM_SUBMISSION_BINS: "283"
  ARTIFACT_RETENTION_DAYS: "7"

jobs:
  notebook-lint:
    name: Lint notebooks (nbqa ruff/black) + output hygiene
    runs-on: ubuntu-latest
    timeout-minutes: 12
    permissions: { contents: read }
    steps:
      - uses: actions/checkout@3df4ab11eba7bda6032a0b82a6bb43b11571feac # v4

      - uses: actions/setup-python@0b93645e9fea7318efd9b4012f1f1a1f4f6c21fc # v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip
          cache-dependency-path: |
            requirements-kaggle.txt
            requirements.txt
            requirements-dev.txt
            pyproject.toml

      - name: Install lint deps
        run: |
          python -m pip install --upgrade pip
          pip install "nbqa>=1,<2" "ruff>=0.5,<1.0" "black>=24,<25" nbstripout jq

      - name: nbqa ruff
        run: nbqa ruff notebooks --output-format=github

      - name: nbqa black --check
        run: nbqa black --check notebooks

      - name: Enforce stripped outputs (warn-only)
        continue-on-error: true
        run: |
          set -euo pipefail
          while IFS= read -r -d '' nb; do
            if jq -e '.. | .outputs? // empty | length > 0' "$nb" >/dev/null; then
              echo "::warning file=$nb::Notebook contains cell outputs; consider stripping (nbstripout)."
            fi
          done < <(git ls-files 'notebooks/**/*.ipynb' -z || true)

  local-validate:
    name: Execute notebooks (offline-like; CPU) + param sweeps
    runs-on: ubuntu-latest
    timeout-minutes: 55
    needs: notebook-lint
    permissions: { contents: read }
    steps:
      - name: Checkout
        uses: actions/checkout@3df4ab11eba7bda6032a0b82a6bb43b11571feac # v4
        with: { fetch-depth: 0 }

      - name: Set up Python
        uses: actions/setup-python@0b93645e9fea7318efd9b4012f1f1a1f4f6c21fc # v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip
          cache-dependency-path: |
            requirements-kaggle.txt
            requirements.txt
            requirements-dev.txt
            pyproject.toml
            setup.cfg
            setup.py

      - name: Install minimal Kaggle-friendly deps
        run: |
          python -m pip install --upgrade pip wheel
          if [ -f requirements-kaggle.txt ]; then pip install -r requirements-kaggle.txt; fi
          python -m pip install jupyter nbconvert nbclient nbformat papermill ipykernel jq
          # common numerics (quiet failure is fine on GH-hosted)
          python -m pip install -q numpy pandas matplotlib || true
          # local package for imports, best-effort
          if [ -f pyproject.toml ] || [ -f setup.py ]; then pip install -e . || true; fi
          python -m ipykernel install --user --name=ci-kernel || true

      - name: Snapshot environment (debug/repro)
        run: |
          mkdir -p .ci_run/env
          python -V > .ci_run/env/python.txt
          pip freeze > .ci_run/env/pip-freeze.txt
          uname -a > .ci_run/env/uname.txt || true

      - name: Offline guard: warn on network/pip usage
        continue-on-error: true
        run: |
          set -euo pipefail
          hits=$(git ls-files 'notebooks/**/*.ipynb' | xargs -I{} jq -r '..|.source? // empty|join("\n")' {} \
            | grep -Ein 'pip +install|apt-get|conda +install|wget |curl |requests\.' || true)
          if [ -n "$hits" ]; then
            echo "$hits" | sed 's/^/::warning::Potential network/pip usage: /'
          fi

      - name: Select notebooks for validation
        id: select
        shell: bash
        run: |
          set -euo pipefail
          LIST_FILE="kaggle/ci_notebooks.txt"
          WANT_HEAVY="${{ github.event.inputs.include_heavy || 'false' }}"
          pick_default() {
            git ls-files 'notebooks/**.ipynb' | grep -E '(00_|01_|10_|smoke|quick|sample|mini).*\.ipynb$' || true
          }
          if [ -f "$LIST_FILE" ]; then
            mapfile -t NB < <(grep -v '^\s*#' "$LIST_FILE" | sed 's/#.*$//' | sed '/^\s*$/d')
          else
            mapfile -t NB < <(pick_default)
            [ "${#NB[@]}" -eq 0 ] && mapfile -t NB < <(git ls-files 'notebooks/**.ipynb' | head -n 1 || true)
          fi
          # Drop notebooks tagged heavy unless include_heavy=true (tag via top-level metadata: {"heavy": true})
          if [ "${WANT_HEAVY,,}" != "true" ]; then
            TMP=()
            for nb in "${NB[@]}"; do
              if jq -e '.metadata.heavy == true' "$nb" >/dev/null 2>&1; then
                echo "::notice file=$nb::Skipping heavy notebook (enable via workflow_dispatch: include_heavy=true)."
              else
                TMP+=("$nb")
              fi
            done
            NB=("${TMP[@]}")
          fi
          printf 'files=%s\n' "$(printf '%s ' "${NB[@]}")" >> "$GITHUB_OUTPUT"

      - name: Discover param files (Papermill)
        id: params
        shell: bash
        run: |
          set -euo pipefail
          mapfile -t P < <(git ls-files 'kaggle/params/**/*.json' 'kaggle/params/**/*.yml' 'kaggle/params/**/*.yaml' 2>/dev/null || true)
          if [ "${#P[@]}" -gt 0 ]; then
            printf 'param_files=%s\n' "$(printf '%s ' "${P[@]}")" >> "$GITHUB_OUTPUT"
          else
            echo "param_files=" >> "$GITHUB_OUTPUT"
          fi

      - name: Execute notebooks (copy; HTML preview; base run)
        if: steps.select.outputs.files != ''
        shell: bash
        env:
          KAGGLE_OFFLINE: ${{ env.KAGGLE_OFFLINE }}
          KAGGLE_KERNEL_RUN_TYPE: "Batch"
          KAGGLE_URL_BASE: "https://www.kaggle.com"
          PYTHONWARNINGS: "ignore"
        run: |
          set -euo pipefail
          mkdir -p .ci_run/notebooks_html
          IFS=' ' read -r -a NB <<< "${{ steps.select.outputs.files }}"
          for src in "${NB[@]}"; do
            base="$(basename "$src")"
            out=".ci_run/${base}"
            cp "$src" "$out"
            echo "::group::Execute $src"
            jupyter nbconvert --execute --to notebook --inplace "$out" --ExecutePreprocessor.timeout=1500
            jupyter nbconvert --to html "$out" --output ".ci_run/notebooks_html/${base%.ipynb}.html"
            echo "::endgroup::"
          done

      - name: Execute param sweeps (Papermill) if any
        if: steps.params.outputs.param_files != ''
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p .ci_run/params_html
          IFS=' ' read -r -a NB <<< "${{ steps.select.outputs.files }}"
          IFS=' ' read -r -a P  <<< "${{ steps.params.outputs.param_files }}"
          for src in "${NB[@]}"; do
            stem="$(basename "$src" .ipynb)"
            for pf in "${P[@]}"; do
              tag="$(basename "${pf%.*}")"
              out=".ci_run/${stem}__${tag}.ipynb"
              echo "::group::Papermill $src with $pf"
              papermill "$src" "$out" -f "$pf"
              jupyter nbconvert --to html "$out" --output ".ci_run/params_html/${stem}__${tag}.html"
              echo "::endgroup::"
            done
          done

      - name: Validate produced submissions (CSV/JSON if present)
        if: always()
        run: |
          python - <<'PY'
          import glob, json, sys, os, pandas as pd
          N = int(os.environ.get("SM_SUBMISSION_BINS", "283"))
          mu   = [f"mu_{i:03d}" for i in range(N)]
          sig  = [f"sigma_{i:03d}" for i in range(N)]
          cols = ["sample_id"] + mu + sig
          errs = 0
          for f in glob.glob("**/submission*.csv", recursive=True):
            try:
              df = pd.read_csv(f)
              miss = [c for c in cols if c not in df.columns]
              if miss: print(f"::error file={f}::Missing columns: {miss[:8]}{' …' if len(miss)>8 else ''}"); errs += 1
              if df.isna().any().any(): print(f"::error file={f}::Contains NaN values"); errs += 1
              if any(c in df for c in sig) and (df[[c for c in sig if c in df]] < 0).any().any():
                print(f"::error file={f}::Negative sigma values"); errs += 1
            except Exception as e:
              print(f"::error file={f}::CSV validation error: {e}"); errs += 1
          for f in glob.glob("**/submission*.json", recursive=True):
            try:
              obj = json.load(open(f))
              if not isinstance(obj, dict): print(f"::error file={f}::JSON must be an object"); errs += 1; continue
              miss = [c for c in cols if c not in obj]
              if miss: print(f"::error file={f}::Missing keys: {miss[:8]}{' …' if len(miss)>8 else ''}"); errs += 1
              for k in sig:
                v = obj.get(k, 0)
                if isinstance(v, (int,float)) and v < 0:
                  print(f"::error file={f}::Negative sigma at {k}"); errs += 1; break
            except Exception as e:
              print(f"::error file={f}::JSON validation error: {e}"); errs += 1
          if errs: sys.exit(1)
          print("Submission validation OK (no files or all valid)")
          PY

      - name: Upload executed notebooks & HTML previews
        if: always()
        uses: actions/upload-artifact@834a144ee995460fba8ed112a2fc961b36a5ec5a # v4
        with:
          name: notebooks-executed
          path: |
            .ci_run/*.ipynb
            .ci_run/notebooks_html/*.html
            .ci_run/params_html/*.html
            .ci_run/env/**
          retention-days: ${{ env.ARTIFACT_RETENTION_DAYS }}

  validate-kaggle-metadata:
    name: Validate Kaggle kernel & dataset metadata
    runs-on: ubuntu-latest
    needs: local-validate
    timeout-minutes: 10
    permissions: { contents: read }
    steps:
      - uses: actions/checkout@3df4ab11eba7bda6032a0b82a6bb43b11571feac # v4

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Validate kernel-metadata.json (if present)
        shell: bash
        run: |
          set -euo pipefail
          files=$(git ls-files 'kaggle/kernels/**/kernel-metadata.json' || true)
          [ -z "$files" ] && { echo "No kernel metadata files found."; exit 0; }
          for f in $files; do
            echo "Checking: $f"
            jq -e '.id and .title and (.is_private|type=="boolean") and (.kernel_type|IN("notebook","script")) and .language' "$f" >/dev/null
            comp_env="${{ env.KAGGLE_COMP }}"
            if [ -n "$comp_env" ]; then
              if ! jq -e --arg c "$comp_env" '.competitionSources? // [] | index($c) != null' "$f" >/dev/null; then
                echo "::notice file=$f::No competitionSources entry for '$comp_env' (ok if intentional)."
              fi
            fi
          done

      - name: Validate dataset-metadata.json (if present)
        shell: bash
        run: |
          set -euo pipefail
          files=$(git ls-files 'kaggle/datasets/**/dataset-metadata.json' || true)
          [ -z "$files" ] && { echo "No dataset metadata files found."; exit 0; }
          for f in $files; do
            echo "Checking: $f"
            jq -e '.id and .title and (.licenses|length>=1)' "$f" >/dev/null
          done

  push-to-kaggle:
    name: Push kernels to Kaggle (guarded)
    runs-on: ubuntu-latest
    needs: [ local-validate, validate-kaggle-metadata ]
    timeout-minutes: 20
    if: ${{ (github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch') && secrets.KAGGLE_USERNAME && secrets.KAGGLE_KEY }}
    permissions: { contents: read }
    steps:
      - name: Checkout
        uses: actions/checkout@3df4ab11eba7bda6032a0b82a6bb43b11571feac # v4

      - name: Set up Python
        uses: actions/setup-python@0b93645e9fea7318efd9b4012f1f1a1f4f6c21fc # v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip
          cache-dependency-path: |
            requirements-kaggle.txt

      - name: Install Kaggle CLI
        run: |
          python -m pip install --upgrade pip
          pip install kaggle jq

      - name: Configure Kaggle credentials
        run: |
          mkdir -p ~/.kaggle
          printf '{"username":"%s","key":"%s"}\n' "${{ secrets.KAGGLE_USERNAME }}" "${{ secrets.KAGGLE_KEY }}" > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json
          kaggle --version

      - name: Discover kernel workspaces
        id: kernels
        shell: bash
        run: |
          set -euo pipefail
          mapfile -t KDIRS < <(git ls-files 'kaggle/kernels/**/kernel-metadata.json' | xargs -n1 dirname 2>/dev/null || true)
          if [ "${#KDIRS[@]}" -eq 0 ]; then
            echo "skip=true" >> "$GITHUB_OUTPUT"
          else
            printf 'dirs=%s\n' "$(printf '%s ' "${KDIRS[@]}")" >> "$GITHUB_OUTPUT"
          fi

      - name: Push kernels
        if: steps.kernels.outputs.skip != 'true'
        shell: bash
        env: { KAGGLE_COMP: ${{ env.KAGGLE_COMP }} }
        run: |
          set -euo pipefail
          IFS=' ' read -r -a KDIRS <<< "${{ steps.kernels.outputs.dirs }}"
          for d in "${KDIRS[@]}"; do
            echo "::group::kaggle kernels push -p $d"
            kaggle kernels push -p "$d" | tee "kaggle_push_$(basename "$d").log"
            echo "::endgroup::"
          done

      - name: Upload Kaggle push logs
        if: always()
        uses: actions/upload-artifact@834a144ee995460fba8ed112a2fc961b36a5ec5a # v4
        with:
          name: kaggle-push-logs
          path: kaggle_push_*.log
          retention-days: ${{ env.ARTIFACT_RETENTION_DAYS }}

  dataset-sync:
    name: Sync datasets to Kaggle (optional)
    runs-on: ubuntu-latest
    needs: [ local-validate, validate-kaggle-metadata ]
    timeout-minutes: 20
    if: ${{ (github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch') && secrets.KAGGLE_USERNAME && secrets.KAGGLE_KEY && hashFiles('kaggle/datasets/**/dataset-metadata.json') != '' }}
    permissions: { contents: read }
    steps:
      - uses: actions/checkout@3df4ab11eba7bda6032a0b82a6bb43b11571feac # v4
      - uses: actions/setup-python@0b93645e9fea7318efd9b4012f1f1a1f4f6c21fc # v5
        with: { python-version: ${{ env.PYTHON_VERSION }} }
      - name: Install Kaggle CLI
        run: |
          python -m pip install --upgrade pip
          pip install kaggle
      - name: Version datasets
        shell: bash
        run: |
          set -euo pipefail
          for meta in $(git ls-files 'kaggle/datasets/**/dataset-metadata.json'); do
            d=$(dirname "$meta")
            echo "::group::kaggle datasets version -p $d"
            kaggle datasets version -p "$d" -m "CI sync: ${GITHUB_SHA::7}" -r zip | tee "kaggle_dataset_$(basename "$d").log"
            echo "::endgroup::"
          done
      - name: Upload dataset logs
        if: always()
        uses: actions/upload-artifact@834a144ee995460fba8ed112a2fc961b36a5ec5a # v4
        with:
          name: kaggle-dataset-logs
          path: kaggle_dataset_*.log
          retention-days: ${{ env.ARTIFACT_RETENTION_DAYS }}

name: kaggle-notebook-ci

on:
  push:
    branches: [main, develop]
    paths:
      - "notebooks/**"
      - "kaggle/**"
      - "src/**"
      - "configs/**"
      - "requirements-kaggle.txt"
      - ".github/workflows/kaggle_notebook_ci.yml"
  pull_request:
    branches: [main, develop]
    paths:
      - "notebooks/**"
      - "kaggle/**"
      - "src/**"
      - "configs/**"
      - "requirements-kaggle.txt"
      - ".github/workflows/kaggle_notebook_ci.yml"
  workflow_dispatch:

permissions:
  contents: read

concurrency:
  group: kaggle-notebook-ci-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.11"
  KAGGLE_COMP: "ariel-data-challenge-2025"   # notebooks may override
  KAGGLE_OFFLINE: "1"                        # emulate Kaggle's no-internet
  PYTHONHASHSEED: "0"
  MPLBACKEND: "Agg"

jobs:
  notebook-lint:
    name: Lint notebooks (nbqa ruff/black) + output hygiene
    runs-on: ubuntu-latest
    timeout-minutes: 12
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"
          cache-dependency-path: |
            requirements-kaggle.txt
            requirements.txt
            requirements-dev.txt
            pyproject.toml

      - name: Install lint deps
        run: |
          python -m pip install --upgrade pip
          pip install "nbqa>=1,<2" "ruff>=0.5,<1.0" "black>=24,<25" nbstripout jq

      - name: nbqa ruff
        run: nbqa ruff notebooks --output-format=github

      - name: nbqa black --check
        run: nbqa black --check notebooks

      - name: Enforce stripped outputs (warn-only)
        continue-on-error: true
        run: |
          set -euo pipefail
          while IFS= read -r -d '' nb; do
            if jq -e '.. | .outputs? // empty | length > 0' "$nb" >/dev/null; then
              echo "::warning file=$nb::Notebook contains cell outputs; consider stripping (nbstripout)."
            fi
          done < <(git ls-files 'notebooks/**/*.ipynb' -z || true)

  local-validate:
    name: Execute notebooks (offline-like; CPU)
    runs-on: ubuntu-latest
    timeout-minutes: 40
    needs: notebook-lint
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"
          cache-dependency-path: |
            requirements-kaggle.txt
            requirements.txt
            requirements-dev.txt
            pyproject.toml
            setup.cfg
            setup.py

      - name: Install minimal Kaggle-friendly deps
        run: |
          python -m pip install --upgrade pip wheel
          if [ -f requirements-kaggle.txt ]; then pip install -r requirements-kaggle.txt; fi
          pip install jupyter nbconvert nbclient nbformat papermill ipykernel jq
          pip install -q numpy pandas matplotlib || true
          if [ -f pyproject.toml ] || [ -f setup.py ]; then pip install -e .; fi

      - name: Select notebooks for validation
        id: select
        shell: bash
        run: |
          set -euo pipefail
          LIST_FILE="kaggle/ci_notebooks.txt"
          if [ -f "$LIST_FILE" ]; then
            mapfile -t NB < <(grep -v '^\s*#' "$LIST_FILE" | sed 's/#.*$//' | sed '/^\s*$/d')
          else
            mapfile -t NB < <(git ls-files 'notebooks/**.ipynb' | grep -E '(00_|01_|10_|smoke|quick|sample|mini).*\.ipynb$' || true)
            if [ "${#NB[@]}" -eq 0 ]; then
              mapfile -t NB < <(git ls-files 'notebooks/**.ipynb' | head -n 1 || true)
            fi
          fi
          printf 'files=%s\n' "$(printf '%s ' "${NB[@]}")" >> "$GITHUB_OUTPUT"

      - name: Execute notebooks (copy; HTML preview)
        if: steps.select.outputs.files != ''
        shell: bash
        env:
          KAGGLE_OFFLINE: ${{ env.KAGGLE_OFFLINE }}
          PYTHONWARNINGS: "ignore"
        run: |
          set -euo pipefail
          mkdir -p .ci_run/notebooks_html
          IFS=' ' read -r -a NB <<< "${{ steps.select.outputs.files }}"
          for src in "${NB[@]}"; do
            base="$(basename "$src")"
            out=".ci_run/${base}"
            cp "$src" "$out"
            echo "::group::Execute $src"
            jupyter nbconvert --execute --to notebook --inplace "$out" --ExecutePreprocessor.timeout=1200
            jupyter nbconvert --to html "$out" --output ".ci_run/notebooks_html/${base%.ipynb}.html"
            echo "::endgroup::"
          done

      - name: Validate produced submissions (CSV/JSON if present)
        if: always()
        run: |
          python - <<'PY'
          import glob, json, sys, csv, os
          from pathlib import Path
          import pandas as pd
          # Expect 283 mu_*, 283 sigma_* + sample_id (total 567 columns)
          N = int(os.environ.get("SM_SUBMISSION_BINS", "283"))
          expected_mu   = [f"mu_{i:03d}" for i in range(N)]
          expected_sig  = [f"sigma_{i:03d}" for i in range(N)]
          expected_cols = ["sample_id"] + expected_mu + expected_sig
          errors = 0
          # CSVs
          for f in glob.glob("**/submission*.csv", recursive=True):
            try:
              df = pd.read_csv(f)
              miss = [c for c in expected_cols if c not in df.columns]
              if miss:
                print(f"::error file={f}::Missing columns: {miss[:8]}{' ...' if len(miss)>8 else ''}")
                errors += 1
              if df.isna().any().any():
                print(f"::error file={f}::Contains NaN values"); errors += 1
              if (df[[c for c in expected_sig if c in df]].lt(0)).any().any():
                print(f"::error file={f}::Negative sigma found"); errors += 1
            except Exception as e:
              print(f\"::error file={f}::CSV validation error: {e}\"); errors += 1
          # JSON (single-record)
          for f in glob.glob("**/submission*.json", recursive=True):
            try:
              obj = json.load(open(f))
              if not isinstance(obj, dict):
                print(f\"::error file={f}::JSON must be an object\"); errors += 1
                continue
              miss = [c for c in expected_cols if c not in obj]
              if miss:
                print(f\"::error file={f}::Missing keys: {miss[:8]}{' ...' if len(miss)>8 else ''}\"); errors += 1
              for k in expected_sig:
                v = obj.get(k, 0)
                if isinstance(v, (int,float)) and v < 0:
                  print(f\"::error file={f}::Negative sigma at {k}\"); errors += 1; break
            except Exception as e:
              print(f\"::error file={f}::JSON validation error: {e}\"); errors += 1
          if errors:
            sys.exit(1)
          print(\"Submission validation OK (no files or all valid)\")
          PY

      - name: Upload executed notebooks & HTML previews
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: notebooks-executed
          path: |
            .ci_run/*.ipynb
            .ci_run/notebooks_html/*.html
          retention-days: 7

  validate-kaggle-metadata:
    name: Validate Kaggle kernel & dataset metadata
    runs-on: ubuntu-latest
    needs: local-validate
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Validate kernel-metadata.json (if present)
        shell: bash
        run: |
          set -euo pipefail
          files=$(git ls-files 'kaggle/kernels/**/kernel-metadata.json' || true)
          [ -z "$files" ] && { echo "No kernel metadata files found."; exit 0; }
          for f in $files; do
            echo "Checking: $f"
            jq -e '.id and .title and (.is_private|type=="boolean") and (.kernel_type|IN("notebook","script")) and .language' "$f" >/dev/null
            comp_env="${{ env.KAGGLE_COMP }}"
            if [ -n "$comp_env" ]; then
              if ! jq -e --arg c "$comp_env" '.competitionSources? // [] | index($c) != null' "$f" >/dev/null; then
                echo "::notice file=$f::No competitionSources entry for '$comp_env' (ok if intentional)."
              fi
            fi
          done

      - name: Validate dataset-metadata.json (if present)
        shell: bash
        run: |
          set -euo pipefail
          files=$(git ls-files 'kaggle/datasets/**/dataset-metadata.json' || true)
          [ -z "$files" ] && { echo "No dataset metadata files found."; exit 0; }
          for f in $files; do
            echo "Checking: $f"
            jq -e '.id and .title and (.licenses|length>=1)' "$f" >/dev/null
          done

  push-to-kaggle:
    name: Push kernels to Kaggle (guarded)
    runs-on: ubuntu-latest
    needs: [local-validate, validate-kaggle-metadata]
    timeout-minutes: 20
    if: ${{ (github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch') && secrets.KAGGLE_USERNAME && secrets.KAGGLE_KEY }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"
          cache-dependency-path: |
            requirements-kaggle.txt

      - name: Install Kaggle CLI
        run: |
          python -m pip install --upgrade pip
          pip install kaggle jq

      - name: Configure Kaggle credentials
        run: |
          mkdir -p ~/.kaggle
          printf '{"username":"%s","key":"%s"}\n' "${{ secrets.KAGGLE_USERNAME }}" "${{ secrets.KAGGLE_KEY }}" > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json
          kaggle --version

      - name: Discover kernel workspaces
        id: kernels
        shell: bash
        run: |
          set -euo pipefail
          mapfile -t KDIRS < <(git ls-files 'kaggle/kernels/**/kernel-metadata.json' | xargs -n1 dirname 2>/dev/null || true)
          if [ "${#KDIRS[@]}" -eq 0 ]; then
            echo "skip=true" >> "$GITHUB_OUTPUT"
          else
            printf 'dirs=%s\n' "$(printf '%s ' "${KDIRS[@]}")" >> "$GITHUB_OUTPUT"
          fi

      - name: Push kernels
        if: steps.kernels.outputs.skip != 'true'
        shell: bash
        env: { KAGGLE_COMP: ${{ env.KAGGLE_COMP }} }
        run: |
          set -euo pipefail
          IFS=' ' read -r -a KDIRS <<< "${{ steps.kernels.outputs.dirs }}"
          for d in "${KDIRS[@]}"; do
            echo "::group::kaggle kernels push -p $d"
            kaggle kernels push -p "$d" | tee "kaggle_push_$(basename "$d").log"
            echo "::endgroup::"
          done

      - name: Upload Kaggle push logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: kaggle-push-logs
          path: kaggle_push_*.log
          retention-days: 7

  dataset-sync:
    name: Sync datasets to Kaggle (optional)
    runs-on: ubuntu-latest
    needs: [local-validate, validate-kaggle-metadata]
    timeout-minutes: 20
    if: ${{ (github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch') && secrets.KAGGLE_USERNAME && secrets.KAGGLE_KEY && hashFiles('kaggle/datasets/**/dataset-metadata.json') != '' }}
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Kaggle CLI
        run: |
          python -m pip install --upgrade pip
          pip install kaggle

      - name: Version datasets
        shell: bash
        run: |
          set -euo pipefail
          for meta in $(git ls-files 'kaggle/datasets/**/dataset-metadata.json'); do
            d=$(dirname "$meta")
            echo "::group::kaggle datasets version -p $d"
            kaggle datasets version -p "$d" -m "CI sync: ${GITHUB_SHA::7}" -r zip | tee "kaggle_dataset_$(basename "$d").log"
            echo "::endgroup::"
          done

      - name: Upload dataset logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: kaggle-dataset-logs
          path: kaggle_dataset_*.log
          retention-days: 7
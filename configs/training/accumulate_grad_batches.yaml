# ======================================================================
# SpectraMind V50 — Gradient Accumulation Config
# ======================================================================
# Defines how many mini-batches to accumulate before one optimizer step.
# This allows simulation of larger effective batch sizes under GPU/VRAM
# constraints (esp. Kaggle’s 16 GB GPUs).
#
# Usage:
#   spectramind train +training/accumulate_grad_batches=2
#
# Notes:
# - Effective batch size = dataloader.batch_size * accumulate_grad_batches
# - Value = 1 → no accumulation (default).
# - Adjust LR scaling when increasing accumulation (linear scaling rule).
# ======================================================================

# Default accumulation factor
accumulate_grad_batches: ${oc.env:SM_ACCUM, 2}

# ----------------------------------------------------------------------
# Profiles tuned for common environments
# ----------------------------------------------------------------------
profiles:
  kaggle:
    accumulate_grad_batches: ${oc.env:SM_ACCUM_KAGGLE, 2}   # safe for T4 / 16 GB
  local:
    accumulate_grad_batches: ${oc.env:SM_ACCUM_LOCAL, 1}    # no accumulation
  large_gpu:
    accumulate_grad_batches: ${oc.env:SM_ACCUM_LARGE, 4}    # A100/RTX6000 class
  debug:
    accumulate_grad_batches: 1                              # ensure quick feedback

# ----------------------------------------------------------------------
# Hydra bookkeeping
# ----------------------------------------------------------------------
hydra:
  run:
    dir: outputs/training/accum/${now:%Y-%m-%d_%H-%M-%S}

# configs/training/optimizer.yaml
# ======================================================================
# SpectraMind V50 — Optimizer Configuration (Hydra sub-config)
# ======================================================================
# Default: AdamW (decoupled weight decay). Includes:
# - Param groups (no decay for bias/Norm/embeddings)
# - AMP-friendly eps
# - Fused/foreach/capturable toggles (auto-safe)
# - Optional head/mult LR groups for fine-tune
# ======================================================================

# -------------------------
# Selection
# -------------------------
# one of: adamw | adam | sgd | rmsprop | lamb | lion | adafactor | adamax | adagrad | adamw_8bit
name: adamw

# -------------------------
# Base hyperparameters
# -------------------------
lr: 3.0e-4                     # base LR (pre-scheduler)
weight_decay: 0.01
betas: [0.9, 0.999]
eps: 1.0e-8
amsgrad: false

numerics:
  adamw_eps_floor: 1.0e-8      # clamp eps lower bound for Adam-family
  clamp_grad_inf: true         # zero-out NaN/Inf grads defensively

# -------------------------
# PyTorch Lightning integration (mirrors precision.yaml)
# -------------------------
pl:
  grad_clip:
    enabled: true
    algorithm: norm            # "norm" | "value"
    value: 1.0                 # when algorithm == "value"
    max_norm: 5.0              # when algorithm == "norm"
  detect_anomaly: false

# -------------------------
# Torch impl toggles (degrade gracefully if unsupported)
# -------------------------
impl:
  fused: auto                  # true | false | auto (CUDA+torch>=2 prefers fused)
  foreach: auto                # batched per-parameter ops
  capturable: false            # true if using CUDA graphs
  maximize: false              # gradient ascent (rare)
  differentiable: false        # set true for higher-order grads if needed

# -------------------------
# Param-group rules (regex on fully-qualified param names)
# Ensures weight decay is NOT applied to biases/Norm/embeddings.
# Add optional head/backbone LRs for fine-tuning.
# -------------------------
param_groups:
  # Weight-decayed parameters (typical weights/kernels)
  - name: decay
    weight_decay: ${weight_decay}
    lr: ${lr}
    params_regex:
      - "(?i).*\\.(weight|kernel)$"

  # No-decay: biases, norms, embeddings
  - name: no_decay
    weight_decay: 0.0
    lr: ${lr}
    params_regex:
      - "(?i).*\\.(bias)$"
      - "(?i).*(layernorm|rmsnorm|groupnorm|batchnorm|instancenorm|norm)(\\.|_)?(weight|bias)$"
      - "(?i).*\\.(emb(edding)?s?|pos_embed|positional_embedding)(\\.|_)?(weight|bias)?$"
      - "(?i).*\\.(bn)(\\.|_)?(weight|bias)$"

  # Optional: classifier/head gets different LR (multiplier below)
  - name: head
    enabled: false
    lr: ${eval:'${lr} * ${head_lr.multiplier}'}
    weight_decay: ${weight_decay}
    params_regex:
      - "(?i)^(head|classifier|probe)\\."

  # Optional: backbone gets reduced LR
  - name: backbone
    enabled: false
    lr: ${eval:'${lr} * ${backbone_lr.multiplier}'}
    weight_decay: ${weight_decay}
    params_regex:
      - "(?i)^(backbone|encoder)\\."

param_fallback_to_decay: true   # anything unmatched → decay group

head_lr:
  multiplier: 1.0               # e.g., 5.0 to speed up a small head

backbone_lr:
  multiplier: 1.0               # e.g., 0.1 for careful fine-tune

# -------------------------
# Linear/SQRT LR scaling helper (trainer may apply)
# -------------------------
lr_scaling:
  enabled: false
  reference_batch_size: 256
  actual_batch_size: ${trainer.effective_batch_size}   # define in trainer cfg
  mode: linear                                         # "linear" | "sqrt"

# -------------------------
# Variants (targets + defaults)
# -------------------------
variants:

  adamw:
    _target_: torch.optim.AdamW
    lr: ${lr}
    betas: ${betas}
    eps: ${eps}
    weight_decay: ${weight_decay}
    amsgrad: ${amsgrad}
    foreach: ${impl.foreach}
    fused: ${impl.fused}
    capturable: ${impl.capturable}
    maximize: ${impl.maximize}
    differentiable: ${impl.differentiable}

  adam:
    _target_: torch.optim.Adam
    lr: 1.0e-3
    betas: ${betas}
    eps: ${eps}
    weight_decay: 0.0
    amsgrad: false
    foreach: ${impl.foreach}
    capturable: ${impl.capturable}
    maximize: ${impl.maximize}
    differentiable: ${impl.differentiable}

  adamax:
    _target_: torch.optim.Adamax
    lr: 2.0e-3
    betas: ${betas}
    eps: ${eps}
    weight_decay: 0.0

  adagrad:
    _target_: torch.optim.Adagrad
    lr: 1.0e-2
    lr_decay: 0.0
    weight_decay: 0.0
    eps: ${eps}

  sgd:
    _target_: torch.optim.SGD
    lr: 1.0e-2
    momentum: 0.9
    nesterov: true
    weight_decay: 1.0e-4
    maximize: ${impl.maximize}

  rmsprop:
    _target_: torch.optim.RMSprop
    lr: 1.0e-3
    alpha: 0.99
    eps: ${eps}
    momentum: 0.9
    centered: false
    weight_decay: 0.0

  # Requires torch-optimizer in env
  lamb:
    _target_: torch_optimizer.Lamb
    lr: 3.0e-4
    betas: ${betas}
    eps: ${eps}
    weight_decay: ${weight_decay}
    clamp_value: 10.0
    debias: true

  # Requires timm in env
  lion:
    _target_: timm.optim.lion.Lion
    lr: 3.0e-4
    betas: [0.9, 0.99]
    weight_decay: ${weight_decay}
    use_triton: false

  # Transformers Adafactor (great for huge embeddings; no weight_decay here)
  adafactor:
    _target_: transformers.optimization.Adafactor
    lr: 1.0e-3
    scale_parameter: true
    relative_step: false
    warmup_init: false
    weight_decay: 0.0
    eps: [1.0e-30, 1.0e-3]

  # BitsAndBytes 8-bit AdamW (optional; ignored if bnb not installed)
  adamw_8bit:
    _target_: bitsandbytes.optim.AdamW8bit
    lr: ${lr}
    betas: ${betas}
    eps: ${eps}
    weight_decay: ${weight_decay}

# -------------------------
# Safety gates (trainer may enforce each step)
# -------------------------
safety:
  forbid_nan_loss: true
  zero_grad_on_nan: true
  log_step_l2_grad_norm: true
  clip_grad_enabled: ${pl.grad_clip.enabled}
  clip_grad_algorithm: ${pl.grad_clip.algorithm}
  clip_grad_value: ${pl.grad_clip.value}
  clip_grad_max_norm: ${pl.grad_clip.max_norm}

# configs/callbacks/early_stopping.yaml
# ======================================================================
# SpectraMind V50 — Early Stopping Configuration (Lightning 2.x)
# ======================================================================
# Hydra-composable; override via CLI or SM_* env vars.
# Works with configs/training/lightning.yaml and /training/fast_ci.yaml.
# ======================================================================

_target_: pytorch_lightning.callbacks.EarlyStopping

# Toggle (your callback builder should respect this)
_enabled: ${oc.env:SM_ES_ENABLED, true}

# Metric wiring (must be logged by your validation loop)
monitor: ${oc.env:SM_ES_MONITOR, "val/loss"}      # e.g., "val/gll", "val/score"

# Direction of improvement
mode: ${oc.env:SM_ES_MODE, "min"}                 # "min" | "max"

# Sensitivity & patience
min_delta: ${oc.env:SM_ES_MIN_DELTA, 0.0}         # absolute change required
patience:  ${oc.env:SM_ES_PATIENCE, 10}           # epochs (or checks) without improvement

# Robustness
strict: ${oc.env:SM_ES_STRICT, true}              # raise if metric missing
check_finite: ${oc.env:SM_ES_CHECK_FINITE, true}  # stop if metric becomes NaN/Inf
verbose: ${oc.env:SM_ES_VERBOSE, true}

# Optional hard thresholds (None → unused)
# If metric crosses these, stop immediately (regardless of patience).
stopping_threshold:   ${oc.env:SM_ES_STOPPING_THRESH, null}    # e.g., 0.001 for loss
divergence_threshold: ${oc.env:SM_ES_DIVERGENCE_THRESH, null}  # e.g., 10.0  for loss

# Multi-process logging hygiene (DDP)
log_rank_zero_only: ${oc.env:SM_ES_LOG_RANK0, true}

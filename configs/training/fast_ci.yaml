# configs/training/fast_ci.yaml
# ======================================================================
# SpectraMind V50 — Fast profile for CI/Kaggle smoke tests (Upgraded)
# ======================================================================
# Layer this on top of your normal train stack, e.g.:
#   python -m spectramind train +defaults='[/training/fast_ci]'
# or:
#   spectramind train +defaults='[/training/fast_ci]'
#
# Goals:
# - ~2 epochs, FP32, tiny data slices → fast + stable
# - Deterministic kernels for CI reproducibility
# - Minimal checkpointing + quick early-stop
# - Safe dataloader settings for GitHub/Kaggle runners
# ======================================================================

# Keep your logger selection via /logger indirection if present elsewhere.

# Optionally reuse your base callbacks set, then override below.
defaults:
  # If you have a callbacks selector node under training.callbacks, keep it:
  - override /training/callbacks@training.callbacks: callbacks

# ----------------------------- Trainer overrides ------------------------------
trainer:
  # Run length & precision
  max_epochs: 2
  precision: "32-true"                  # Avoid AMP flakiness in CI
  devices: 1
  strategy: "auto"

  # Cadence
  val_check_interval: 1.0               # validate once per epoch
  check_val_every_n_epoch: null
  log_every_n_steps: 10

  # Stability
  accumulate_grad_batches: 1
  gradient_clip_val: 0.5
  gradient_clip_algorithm: "norm"
  benchmark: false
  deterministic: true                   # force deterministic kernels

  # Make the run tiny
  limit_train_batches: 0.1              # 10% of train
  limit_val_batches: 0.1                # 10% of val
  limit_test_batches: 0.1
  limit_predict_batches: 0.1

  # UI/noise
  enable_checkpointing: true
  profiler: null                        # keep off in CI
  # ModelSummary can be heavy; leave off for smoke runs
  enable_model_summary: false

  # Let base config define these; set here for safety if missing
  default_root_dir: ${paths.logs_dir}

# --------------------------- Dataloader perf (CI) -----------------------------
data_loader:
  num_workers: 0          # fully deterministic, avoids fork issues in CI
  pin_memory: false
  persistent_workers: false
  prefetch_factor: 2

# ------------------------------ Paths / identity ------------------------------
paths:
  ckpt_dir: ${oc.env:SM_CKPT_DIR, artifacts/models}
  logs_dir: ${oc.env:SM_LOGS_DIR, artifacts/logs}
  run_name: ${oc.env:SM_RUN_NAME, "ci-${now:%Y%m%d-%H%M%S}"}

# ------------------------------ Quick callbacks -------------------------------
training:
  callbacks:
    callbacks:
      - _target_: pytorch_lightning.callbacks.EarlyStopping
        monitor: "val/loss"
        mode: "min"
        patience: 2
        min_delta: 0.0
        strict: true
        verbose: true

      - _target_: pytorch_lightning.callbacks.ModelCheckpoint
        dirpath: "${paths.ckpt_dir}"
        filename: "ci-{epoch:02d}-{val/loss:.4f}"
        monitor: "val/loss"
        mode: "min"
        save_top_k: 1
        save_last: false
        auto_insert_metric_name: false

      - _target_: pytorch_lightning.callbacks.LearningRateMonitor
        logging_interval: "epoch"
        log_momentum: false

# -------------------------- Small, deterministic optim ------------------------
# These override your base optimizer/scheduler for smoke runs only.
optimizer:
  _target_: torch.optim.AdamW
  lr: 5.0e-4
  weight_decay: 0.0
  betas: [0.9, 0.999]
  eps: 1.0e-8

scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${trainer.max_epochs}
  eta_min: 1.0e-5

# ------------------------------- Reproducibility ------------------------------
seed: ${oc.env:SM_SEED, 1337}

# ------------------------------- Hydra runtime --------------------------------
defaults:
  - override hydra/job_logging: disabled
  - override hydra/hydra_logging: disabled

hydra:
  run:
    dir: outputs/train-ci/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: multirun/train-ci/${now:%Y-%m-%d_%H-%M-%S}
    subdir: ${hydra.job.num}

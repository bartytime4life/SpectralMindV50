# configs/training/fast_ci.yaml
# ======================================================================
# SpectraMind V50 — Fast profile for CI/Kaggle smoke tests (Upgraded)
# ======================================================================
# Layer this on top of your normal train stack, e.g.:
#   spectramind train +defaults='[/training/fast_ci]'
# Goals:
# - ~2 epochs, FP32, tiny data slices → fast + stable
# - Deterministic kernels for CI reproducibility
# - Minimal checkpointing + quick early-stop
# - Safe dataloader settings for GitHub/Kaggle runners
# ======================================================================

# Keep your logger selection via the training/logger selector elsewhere.

# If you use a callbacks selector node, you can override it here.
# Otherwise we override the top-level callbacks section directly.
# (Safe to keep; harmless if the node doesn't exist.)
defaults:
  - override /training/logger: logger
  # Uncomment if you have a callbacks selector:
  # - override /training/callbacks@training.callbacks: callbacks

# ----------------------------- Trainer overrides ------------------------------
trainer:
  # Run length & precision
  max_epochs: 2
  precision: "32-true"                  # Avoid AMP flakiness in CI
  accelerator: "auto"
  devices: 1
  strategy: "auto"

  # Cadence
  val_check_interval: 1.0               # validate once per epoch
  check_val_every_n_epoch: null
  log_every_n_steps: 10

  # Stability
  accumulate_grad_batches: 1
  gradient_clip_algorithm: "norm"
  gradient_clip_val: 0.5
  benchmark: false
  deterministic: true                   # force deterministic kernels

  # Make the run tiny
  limit_train_batches: 0.1              # 10% of train
  limit_val_batches: 0.1                # 10% of val
  limit_test_batches: 0.1
  limit_predict_batches: 0.1

  # UI/noise
  enable_checkpointing: true
  profiler: null                        # keep off in CI

  # Root dir fallback (base config defines paths)
  default_root_dir: ${paths.logs_dir}

# --------------------------- Dataloader perf (CI) -----------------------------
data_loader:
  num_workers: 0          # fully deterministic, avoids fork issues in CI
  pin_memory: false
  persistent_workers: false
  prefetch_factor: 2

# ------------------------------ Paths / identity ------------------------------
paths:
  ckpt_dir: ${oc.env:SM_CKPT_DIR, artifacts/models}
  logs_dir: ${oc.env:SM_LOGS_DIR, artifacts/logs}
  run_name: ${oc.env:SM_RUN_NAME, "ci-${now:%Y%m%d-%H%M%S}"}

# ------------------------------ Quick callbacks -------------------------------
# This replaces/overrides your base callbacks for smoke runs.
callbacks:
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val/loss"
    mode: "min"
    patience: 2
    min_delta: 0.0
    verbose: true

  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: ${paths.ckpt_dir}
    filename: "ci-{epoch:02d}-{val/loss:.4f}"
    monitor: "val/loss"
    mode: "min"
    save_top_k: 1
    save_last: false
    auto_insert_metric_name: false

  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: "epoch"
    log_momentum: false

# -------------------------- Small, deterministic optim ------------------------
# These **override** your base optimizer/scheduler configs via your selectors.
# (No raw _target_ here; we reuse your existing factories.)
optimizer:
  name: adamw
  lr: 5.0e-4
  weight_decay: 0.0
  betas: [0.9, 0.999]
  eps: 1.0e-8
  amsgrad: false
  impl:
    fused: false           # keep it simple/stable in CI
    foreach: false

scheduler:
  name: cosine_annealing
  pl:
    interval: epoch
    frequency: 1
    monitor: val/loss
    strict: true
  warmup:
    enabled: false         # save time in CI
  cosine_annealing:
    T_max: ${trainer.max_epochs}
    eta_min: 1.0e-5
  limits:
    lr_min: 1.0e-7
    lr_max: ${optimizer.lr}

# ------------------------------- Reproducibility ------------------------------
seed: ${oc.env:SM_SEED,1337}

# ------------------------------- Hydra runtime --------------------------------
defaults:
  - override hydra/job_logging: disabled
  - override hydra/hydra_logging: disabled

hydra:
  run:
    dir: outputs/train-ci/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: multirun/train-ci/${now:%Y-%m-%d_%H-%M-%S}
    subdir: ${hydra.job.num}

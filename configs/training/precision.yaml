# configs/training/precision.yaml
# ======================================================================
# SpectraMind V50 — Precision / Numerical Config (Lightning 2.x safe)
# ======================================================================
# Controls mixed precision, gradient scaling, determinism, TF32, matmul,
# and safety guards. Defaults are Kaggle-GPU friendly (T4/V100/AMP).
# ======================================================================

precision:
  # Trainer.precision (map directly in Trainer factory)
  # "32-true" | "16-mixed" | "bf16-mixed" | "64-true"
  mode: ${oc.env:SM_PRECISION,"16-mixed"}

  # -------------------------
  # Autocast (effective when mode ends with "-mixed")
  # -------------------------
  autocast:
    enabled: true
    # dtype inferred from mode; override only for expert use:
    # "float16" | "bfloat16" | null (auto)
    dtype: null
    cache_enabled: true
    # Modules/ops to exclude from autocast (numerically sensitive)
    denylist_modules: []   # e.g., ["spectramind.models.decoders.Heteroscedastic"]
    # Force CPU autocast off (CUDA runs handle most gains)
    cpu_enabled: false

  # -------------------------
  # Gradient Scaler (used by fp16 mixed precision; ignored by bf16)
  # -------------------------
  grad_scaler:
    enabled: true          # harmless if bf16; Lightning no-ops as needed
    init_scale: 65536      # 2**16
    growth_interval: 2000
    backoff_factor: 0.5
    hysteresis: 2
    enabled_on_cpu: false

  # -------------------------
  # Determinism & Repro Seeds
  # -------------------------
  determinism:
    torch_use_deterministic_algorithms: true
    cudnn_benchmark: false
    cudnn_deterministic: true
    # Stable hashing across Hydra sweeps & workers
    python_hash_seed: ${hydra:job.num,42}
    seed: ${oc.env:SM_SEED,42}

  # -------------------------
  # TF32 (Ampere+). Keep OFF until you validate numerics.
  # -------------------------
  tf32:
    allow: false           # torch.backends.cuda.matmul.allow_tf32
    cudnn_allow: false     # torch.backends.cudnn.allow_tf32

  # -------------------------
  # PyTorch 2.x matmul precision policy
  # -------------------------
  matmul:
    precision: "highest"   # "highest" | "high" | "medium"

  # -------------------------
  # Overflow / Anomaly Guards
  # -------------------------
  safety:
    nan_guard: true        # assert/raise on NaN/Inf in loss or grads
    detect_anomaly: false  # VERY slow; enable for pinpoint debugging only
    overflow_recovery:
      enabled: true
      steps:
        - zero_grad        # purge contaminated grads
        - skip_step        # skip optimizer step for this batch
        # Add "reduce_lr" here if you implement a reactive LR drop

  # -------------------------
  # Gradient Clipping (map into Trainer)
  # -------------------------
  grad_clip:
    enabled: true
    algorithm: "norm"      # "norm" | "value"
    value: 1.0             # for algorithm: "value"
    max_norm: 5.0          # for algorithm: "norm"

  # -------------------------
  # Zero-grad behavior
  # -------------------------
  zero_grad:
    set_to_none: true

# -------------------------
# Precision-adjacent options (enable in training loop)
# -------------------------
adjacent:
  activation_checkpointing:
    enabled: false
    modules: []  # e.g., ["spectramind.models.fgs1.MambaBlock", "spectramind.models.airs.GNNLayer"]

  ema:
    enabled: false
    decay: 0.999
    device: "cpu"          # "cpu" | "auto"
    pin_memory: true

  swa:
    enabled: false
    start_epoch: 0.8       # fraction of max_epochs (0.8 → last 20%)
    anneal_epochs: 5
    lr: null               # null → reuse optimizer LR

# -------------------------
# Runtime clamps (Trainer should apply after every LR compute)
# -------------------------
limits:
  lr_min: 1.0e-7
  lr_max: ${optimizer.lr}

# --------------------------------------------------------------------------------------
# Integration notes (wire these in your Trainer factory):
#   trainer.precision                 ← ${precision.mode}
#   trainer.gradient_clip_algorithm   ← ${precision.grad_clip.algorithm}
#   trainer.gradient_clip_val         ← ${precision.grad_clip.max_norm} (if norm)
#   torch.backends.cuda.matmul.allow_tf32   = ${precision.tf32.allow}
#   torch.backends.cudnn.allow_tf32         = ${precision.tf32.cudnn_allow}
#   torch.set_float32_matmul_precision(${precision.matmul.precision})
#   torch.use_deterministic_algorithms(${precision.determinism.torch_use_deterministic_algorithms})
#   torch.backends.cudnn.benchmark     = ${precision.determinism.cudnn_benchmark}
#   torch.backends.cudnn.deterministic = ${precision.determinism.cudnn_deterministic}
#   GradScaler config: apply only when mode == "16-mixed" (safe to pass otherwise)
#   NaN guard: add hooks/asserts around loss.backward() and post-optimizer checks.
# --------------------------------------------------------------------------------------

# Hydra runtime noise floor
hydra:
  job_logging:
    root:
      level: WARN
  run:
    dir: outputs/training/precision/${now:%Y-%m-%d_%H-%M-%S}

# ==============================================================================
# SpectraMind V50 â€” Profile: AMP OFF (Force FP32 everywhere)
# ------------------------------------------------------------------------------
# Usage:
#   python -m spectramind train +defaults='[/profiles/amp_off]'
#
# Purpose:
#   - Disables Automatic Mixed Precision (AMP).
#   - Forces true FP32 for all computations.
#   - Ensures reproducibility and rules out AMP-related instabilities.
# ==============================================================================

defaults:
  - /training/lightning: lightning   # base trainer setup
  - /training/precision: precision   # precision schema/overrides

# Trainer-level overrides (PyTorch Lightning integration)
trainer:
  precision: "32-true"   # enforce full FP32
  detect_anomaly: false  # keep safe defaults
  benchmark: false       # reproducibility > speed

# Precision config (Hydra schema override)
precision:
  mode: "32-true"        # force true FP32
  tf32:
    allow: false         # disable TF32 matrix math
    cudnn_allow: false   # disable TF32 in cuDNN

# Experiment metadata for filtering/tracking
experiment:
  name: ${oc.env:SM_EXPERIMENT_NAME, "amp-off"}
  tags: ${oc.env:SM_TAGS, ["profile:amp_off", "precision=fp32", "amp=off"]}

# Ampere-class GPUs (A100/L4/4090): bf16 + TF32 for speed (validate numerics!)
defaults:
  - /training/lightning: lightning
  - /training/precision: precision
  - /training/optimizer: optimizer
  - /training/scheduler: scheduler
  - /training/dataloader: dataloader
  - /training/logger: logger
  - /training/callbacks: callbacks
  - /training/accumulate: accumulate

precision:
  mode: ${oc.env:SM_PRECISION,"bf16-mixed"}
  tf32:
    allow: ${oc.env:SM_TF32_ALLOW,true}
    cudnn_allow: ${oc.env:SM_TF32_CUDNN,true}
  matmul:
    precision: ${oc.env:SM_MATMUL_PRECISION,"high"}  # bf16 autocast + TF32 kernels

scheduler:
  warmup:
    enabled: ${oc.env:SM_WARMUP,true}
    steps: ${oc.env:SM_WARMUP_STEPS,200}

data_loader:
  num_workers: ${oc.env:SM_WORKERS,4}
  persistent_workers: true
  prefetch_factor: 4
  pin_memory: true
  pin_memory_device: "cuda"

experiment:
  tags: ${oc.env:SM_TAGS, ["profile:ampere_bf16","tf32:on"]}

# configs/predict.yaml
# ──────────────────────────────────────────────────────────────────────────────
# SpectraMind V50 — Inference / Prediction Config (Hydra 1.x / PL 2.x)
# Batch prediction & submission formatting.
#
# Examples:
#   python -m spectramind predict checkpoint.path=artifacts/checkpoints/last.ckpt
#   python -m spectramind predict precision=16-mixed submission.format=wide_csv
#   python -m spectramind predict tta.enabled=true tta.n=8 postprocess.smooth_mu.enabled=true
# ──────────────────────────────────────────────────────────────────────────────

defaults:
  - env: local              # or kaggle / debug
  - data: kaggle            # dataset profile for test-set loading
  - calib: nominal          # if inference needs calibrated tensors
  - model: v50              # model architecture used at train time
  - logger: jsonl           # logging backend (jsonl/csv/wandb)
  - override hydra/job_logging: disabled
  - override hydra/hydra_logging: disabled

# High-level experiment metadata
experiment:
  name: "spectramind_v50_predict"
  tags: ["v50", "predict", "submission"]
  notes: ""

# Canonical paths
paths:
  project_root: ${hydra:runtime.cwd}
  artifacts_root: ${paths.project_root}/artifacts
  runs: ${paths.artifacts_root}/runs
  logs: ${hydra:run.dir}/logs
  outputs: ${paths.artifacts_root}/predictions
  checkpoints: ${paths.artifacts_root}/checkpoints
  schema_dir: ${paths.project_root}/schemas

# Runtime controls
runtime:
  seed: 42
  deterministic: true           # torch.use_deterministic_algorithms when possible
  allow_tf32: false             # set true only after numeric validation
  device: "auto"                # "auto" | "cuda:0" | "cpu" | "mps"
  precision: "32-true"          # "32-true" | "16-mixed" | "bf16-mixed"
  inference_mode: true          # torch.inference_mode() wrapper
  cudnn_benchmark: false        # for throughput, set true if determinism retained

# DataLoader (uses env defaults)
dataloader:
  batch_size: 32
  num_workers: ${env.num_workers}
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2
  drop_last: false

# Checkpoint selection & loading
checkpoint:
  # Primary checkpoint (override via CLI: +checkpoint.path=/path/to.ckpt)
  path: ${paths.checkpoints}/last.ckpt
  strict: true                  # require exact key match unless migrating
  fail_if_missing: true         # hard fail when path doesn't exist
  # Optional fallback search if `path` missing; picks newest by mtime
  fallback_glob:
    enabled: true
    pattern: "${paths.checkpoints}/**/*.ckpt"
    recursive: true

# Test-time augmentation (TTA)
tta:
  enabled: false
  n: 4                          # number of stochastic draws
  flips: false                  # dataset-defined temporal/spectral flips
  jitter:
    enabled: false
    sigma: 0.005
  seed_offset: 1000             # ensures distinct RNG streams per draw
  # Aggregation across TTA draws
  aggregate:
    mu: "mean"                  # "mean" | "median"
    sigma: "var_mean_law"       # combines per-draw (mu, sigma) by variance-mean law

# Ensemble across multiple checkpoints
ensemble:
  enabled: false
  checkpoints: []               # e.g. ["${paths.checkpoints}/fold1.ckpt", "..."]
  # Aggregation across members
  aggregate:
    mu: "mean"                  # "mean" | "median"
    sigma: "var_mean_law"

# Post-processing safety & priors
postprocess:
  clamp_nonneg_sigma: true      # enforce σ >= 0
  epsilon_sigma: 1.0e-9         # σ lower bound to avoid degenerate GLL
  enforce_nonneg_mu: false      # set true if strictly positive targets
  smooth_mu:
    enabled: false
    method: "laplacian"         # "laplacian" | "savitzky_golay"
    weight: 0.05
    window: 7                   # used for Savitzky–Golay
    polyorder: 2

# Submission formatting & validation
submission:
  validate_schema: true
  schema_path: ${paths.schema_dir}/submission.schema.json   # (id, mu[283], sigma[283])
  # Supported exporters: "jsonl" (long) | "wide_csv" (wide)
  format: "jsonl"

  jsonl:
    filename: "submission.jsonl"
    indent: 0                  # pretty=2 for debugging; 0 for speed/size
    # JSONL record keys
    keys:
      id: "id"
      mu: "mu"
      sigma: "sigma"

  wide_csv:
    filename: "submission.csv"
    mu_prefix: "mu_"
    sigma_prefix: "sigma_"
    digits: 6
    include_header: true
    # Column order guard for reproducibility
    enforce_column_order: true

# Optional offline metrics (when holdout labels exist)
metrics:
  compute_offline: false
  gll:
    enabled: true
  summary:
    save_json: true
    path: ${paths.outputs}/metrics.json

# Diagnostics artifacts (plots & quick summaries)
diagnostics:
  enabled: true
  out_dir: ${hydra:run.dir}/diagnostics
  plots:
    per_bin_mu_hist: true
    per_bin_sigma_hist: true
    mu_vs_sigma_scatter: true
  save_csv_summary: true

# Logging backends
logger:
  jsonl:
    log_dir: ${paths.logs}/jsonl
    append: true
  csv:
    save_dir: ${paths.logs}/csv
    filename: "predict_metrics.csv"
  wandb:
    project: "spectramind-v50"
    entity: null
    mode: disabled
    tags: ${experiment.tags}
    notes: ${experiment.notes}

# Hydra runtime
hydra:
  run:
    dir: ${paths.runs}/${now:%Y-%m-%d_%H-%M-%S}_predict
  sweep:
    dir: ${paths.artifacts_root}/sweeps
    subdir: ${hydra.job.num}
  job:
    name: ${experiment.name}

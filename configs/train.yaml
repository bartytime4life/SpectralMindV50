# ──────────────────────────────────────────────────────────────────────────────
# SpectraMind V50 — Training Config (Hydra 1.x / PyTorch Lightning 2.x)
# Top-level composed configuration for model training.
#
# Usage (examples):
#   python -m spectramind train
#   python -m spectramind train env=kaggle precision=bf16-mixed trainer.max_epochs=30
#   python -m spectramind train loss=composite loss.composite.weights.nonneg=0.7
#   python -m spectramind train training/callbacks@training.callbacks=callbacks_fast
# ──────────────────────────────────────────────────────────────────────────────

# Composition: swap any group at CLI
defaults:
  - env: local                        # or: kaggle / ci / debug
  - data: nominal                     # dataset profile (nominal/kaggle/splits)
  - calib: nominal                    # calibration pipeline settings
  - model: v50                        # model architecture (dual encoders + decoder)
  - training: lightning               # base PL Trainer params
  - training/callbacks@training.callbacks: callbacks   # bundle of callbacks
  - loss: composite                   # physics-informed composite loss
  - logger: jsonl                     # logging backend (jsonl/csv/wandb)
  - override hydra/job_logging: disabled
  - override hydra/hydra_logging: disabled

# High-level experiment metadata
experiment:
  name: "spectramind_v50_train"
  tags: ["v50", "train", "fgs1", "airs"]
  notes: ""                           # free-form; can be overridden from CLI

# Canonical paths (ENV-DRIVEN for portability: local/CI/Kaggle)
paths:
  data_root: ${env.data_root}
  artifacts_root: ${env.artifacts_root}
  runs: ${paths.artifacts_root}/runs
  logs: ${hydra:run.dir}/logs
  checkpoints: ${hydra:run.dir}/checkpoints
  reports: ${hydra:run.dir}/reports
  cache: ${paths.artifacts_root}/cache
  manifests: ${paths.artifacts_root}/manifests

# Reproducibility & backend knobs
seed: ${env.reproducibility.seed}
reproducibility:
  torch_deterministic_algorithms: ${env.reproducibility.deterministic}  # torch.use_deterministic_algorithms
  cudnn_benchmark: false                     # keep deterministic throughput stable
  cudnn_deterministic: ${env.reproducibility.deterministic}
  allow_tf32: false                          # set true only after validating numerics on your GPU
  python_hash_seed: ${seed}                  # PYTHONHASHSEED alignment

# Precision (override via CLI or env overrides; bf16 preferred on A100)
precision: "16-mixed"   # "32-true" | "16-mixed" | "bf16-mixed"

# Optional PyTorch 2.x compiler (TorchDynamo); default off
compile:
  enabled: false
  mode: "default"        # "default" | "reduce-overhead" | "max-autotune"
  fullgraph: false
  dynamic: false

# Data pipeline (defaults; dataset group may override)
data:
  batch_size:
    train: 64
    val: 64
    test: 64
  num_workers: ${env.num_workers}
  pin_memory: true        # safe default; harmless on CPU
  persistent_workers: true
  prefetch_factor: 2
  drop_last: true
  augment:
    enabled: true
    jitter: 0.01
    dropout: 0.0
  normalization:
    enabled: true
    method: "zscore"       # "minmax" | "none"

# PyTorch Lightning Trainer (PL 2.x)
trainer:
  max_epochs: 50
  min_epochs: 1
  accelerator: ${env.device}       # "cuda" | "cpu" (env-driven)
  devices: 1
  strategy: auto                   # "auto" | "ddp" | "ddp_find_unused_parameters_false" etc.
  precision: ${precision}
  accumulate_grad_batches: 2
  gradient_clip_val: 1.0
  gradient_clip_algorithm: "norm"
  log_every_n_steps: 50
  val_check_interval: 0.25         # validate 4× per epoch; keep <= 1.0
  check_val_every_n_epoch: null
  deterministic: ${env.reproducibility.deterministic}
  enable_checkpointing: true
  enable_model_summary: true
  detect_anomaly: false
  benchmark: false
  inference_mode: true
  profiler: null
  callbacks: ${training.callbacks.callbacks}

# AMP / GradScaler controls
amp:
  enabled: true
  init_scale: 65536
  growth_interval: 2000

# Optimizer / Scheduler
optimizer:
  name: "AdamW"
  _target_: torch.optim.AdamW
  lr: 3e-4
  weight_decay: 1e-2
  betas: [0.9, 0.999]
  eps: 1e-8

scheduler:
  name: "CosineAnnealingLR"
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${trainer.max_epochs}
  eta_min: 1e-6

# Loggers (select via defaults: jsonl | csv | wandb)
logger:
  jsonl:
    log_dir: ${paths.logs}/jsonl
    append: true
    flush_every: 1
  csv:
    save_dir: ${paths.logs}/csv
    filename: "metrics.csv"
  wandb:
    project: "spectramind-v50"
    entity: null
    mode: disabled           # enable when WANDB_API_KEY is available
    tags: ${experiment.tags}
    notes: ${experiment.notes}
    save_code: true

# Composite physics-informed loss (weights feed src/spectramind/losses/composite.py)
loss:
  composite:
    weights:
      gll: 1.0
      smoothness: 0.2
      nonneg: 0.5
      band_coherence: 0.3
      calibration: 0.5
    params:
      smoothness:
        order: 2
        lambda_max: 0.2
      nonneg:
        margin: 0.0
        penalty: "l2"        # or "hinge"
      band_coherence:
        window: 5
        lambda_max: 0.3
      calibration:
        align_bins: true
        lambda_max: 0.5

# Callback bundle (swappable via defaults)
training:
  callbacks:
    callbacks: []

# Evaluation / reporting passes
evaluation:
  enable_test_after_fit: true
  save_report: true
  report_dir: ${paths.reports}
  figures:
    histograms: true
    scatter_mu_sigma: true
    per_bin_stats: true

# Runtime environment reflection (fed by env group)
env_reflection:
  device: ${env.device}
  num_workers: ${env.num_workers}
  seed: ${seed}

# Hydra runtime (each run lands under env.artifacts_root)
hydra:
  run:
    dir: ${paths.runs}/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: ${paths.artifacts_root}/sweeps
    subdir: ${hydra.job.num}
  job:
    name: ${experiment.name}

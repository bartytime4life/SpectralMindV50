{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad46d1d",
   "metadata": {},
   "source": [
    "# SpectraMind V50 — Kaggle Notebook Template\n",
    "\n",
    "**Purpose**: a safe, reproducible scaffold for running the NeurIPS 2025 Ariel Data Challenge workflows on Kaggle **without internet access**.\n",
    "\n",
    "This template supports:\n",
    "- Environment detection (Kaggle vs local)\n",
    "- Read-only data access at `/kaggle/input`\n",
    "- Optional import of the SpectraMind V50 package if it is available as a Kaggle dataset (`/kaggle/input/spectramind-v50`)\n",
    "- Strict, pinned deps preferred (see `requirements-kaggle.txt` in the repo)\n",
    "- Reproducible config snapshot embedded in the notebook\n",
    "- Optional submission packaging (zip)\n",
    "\n",
    "> Keep heavy work in library/DVC stages. This template is intentionally light, deterministic, and zero-internet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086924d",
   "metadata": {},
   "source": [
    "## 0) Environment & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c66d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, platform, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Determinism\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "# Environment detection\n",
    "IS_KAGGLE = Path('/kaggle/input').exists()\n",
    "COMP_DIR = Path('/kaggle/input/ariel-data-challenge-2025') if IS_KAGGLE else Path('./data')\n",
    "WORK_DIR = Path('/kaggle/working') if IS_KAGGLE else Path('.')\n",
    "print(\"Env:\", \"Kaggle\" if IS_KAGGLE else \"Local\", \"| Python:\", sys.version.split()[0])\n",
    "\n",
    "# Optional: repo-attached dataset with installed package (no internet)\n",
    "SPECTRAMIND_DS = Path('/kaggle/input/spectramind-v50')\n",
    "if IS_KAGGLE and SPECTRAMIND_DS.exists():\n",
    "    sys.path.insert(0, str(SPECTRAMIND_DS / 'src'))\n",
    "    print(\"SpectraMind source path added:\", SPECTRAMIND_DS/'src')\n",
    "\n",
    "# Outputs folder\n",
    "OUT = WORK_DIR / 'outputs'\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Outputs:\", OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b64c30",
   "metadata": {},
   "source": [
    "### (Optional) Symlink Kaggle Inputs into a Repo Layout\n",
    "If you mount your code under `/kaggle/working/spectramind-v50/`, you can symlink Kaggle inputs into `data/raw/...` (zero-copy).\n",
    "\n",
    "Uncomment and adjust paths if you use this workflow (see also the symlink note in your docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a1e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# REPO=/kaggle/working/spectramind-v50\n",
    "# mkdir -p \"$REPO/data/raw\" \"$REPO/data/interim\" \"$REPO/data/processed\" \"$REPO/data/external\" \"$REPO/artifacts\" \"$REPO/models\"\n",
    "# # Competition input\n",
    "# ln -sfn /kaggle/input/ariel-data-challenge-2025 \"$REPO/data/raw/adc2025\"\n",
    "# echo \"Symlinked Kaggle inputs under $REPO/data/raw/adc2025\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e16284",
   "metadata": {},
   "source": [
    "## 1) Config Snapshot (embed minimal JSON for provenance)\n",
    "We write a small config snapshot into `outputs/config_snapshot.json` to guarantee a reproducible record of basic settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"pipeline\": [\"calibrate\", \"predict\"],   # example only\n",
    "    \"model\": {\n",
    "        \"fgs1_encoder\": \"mamba_ssm-lite\",\n",
    "        \"airs_encoder\": \"cnn-lite\",\n",
    "        \"decoder\": \"heteroscedastic-head\"\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"competition\": str(COMP_DIR),\n",
    "        \"bins\": 283\n",
    "    },\n",
    "    \"runtime\": {\n",
    "        \"env\": \"kaggle\" if IS_KAGGLE else \"local\",\n",
    "        \"python\": platform.python_version(),\n",
    "        \"seed\": SEED\n",
    "    }\n",
    "}\n",
    "with open(OUT/'config_snapshot.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(\"Wrote:\", OUT/'config_snapshot.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d817863d",
   "metadata": {},
   "source": [
    "## 2) Data Access (read competition files if present)\n",
    "List a subset of competition files for quick discovery (guarded for portability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285dd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(base: Path, patterns=('.csv', '.parquet', '.json')):\n",
    "    if not base.exists():\n",
    "        return []\n",
    "    out = []\n",
    "    for p in base.rglob('*'):\n",
    "        try:\n",
    "            if p.is_file() and p.suffix.lower() in patterns:\n",
    "                out.append(str(p))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return sorted(out)[:50]\n",
    "\n",
    "inventory = list_files(COMP_DIR)\n",
    "print(\"Sample files:\", len(inventory))\n",
    "for p in inventory[:10]:\n",
    "    print(\"-\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68925e67",
   "metadata": {},
   "source": [
    "## 3) Minimal EDA (guarded)\n",
    "Attempt to read small tables like `train.csv`/`test.csv` if present. All steps are guarded to avoid failures in other environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d92672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def safe_read_csv(p: Path, n=5):\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "        print(p.name, df.shape)\n",
    "        display(df.head(n))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Failed reading\", p, \"->\", e)\n",
    "        return None\n",
    "\n",
    "train_csv = COMP_DIR/'train.csv'\n",
    "test_csv  = COMP_DIR/'test.csv'\n",
    "train_df = safe_read_csv(train_csv) if train_csv.exists() else None\n",
    "test_df  = safe_read_csv(test_csv) if test_csv.exists() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0639e5",
   "metadata": {},
   "source": [
    "## 4) Optional: Import SpectraMind and Run Inference Hooks\n",
    "If you published your SpectraMind repo as a Kaggle Dataset containing `src/spectramind`, this cell will auto-import (zero internet).\n",
    "A simple `notebook_predict` hook can be called to generate a small sample prediction for sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31324fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from spectramind.cli_hooks import notebook_predict  # your repo should provide this hook\n",
    "    HAVE_SM = True\n",
    "    print(\"SpectraMind hooks available.\")\n",
    "except Exception as e:\n",
    "    HAVE_SM = False\n",
    "    print(\"SpectraMind hooks not available:\", e)\n",
    "\n",
    "# Optional sample inference\n",
    "sample_ids = None\n",
    "if 'test_df' in globals() and test_df is not None and 'id' in test_df.columns:\n",
    "    sample_ids = test_df['id'].head(5).tolist()\n",
    "\n",
    "if HAVE_SM:\n",
    "    preds = notebook_predict(\n",
    "        comp_dir=str(COMP_DIR),\n",
    "        config=config,\n",
    "        ids=sample_ids\n",
    "    )\n",
    "    display(preds.head())\n",
    "    out_csv = OUT/'submission.csv'\n",
    "    preds.to_csv(out_csv, index=False)\n",
    "    print(\"Wrote submission to\", out_csv)\n",
    "else:\n",
    "    print(\"No spectramind package — keeping template minimal. \"\n",
    "          \"Attach your repo as a dataset and expose a 'notebook_predict' to enable this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff8a3c",
   "metadata": {},
   "source": [
    "## 5) Submission Packaging Helper (zip)\n",
    "Creates `submission.zip` in the working directory if `outputs/submission.csv` exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def zip_submission(csv_path: Path, zip_path: Path):\n",
    "    assert csv_path.exists(), \"CSV not found\"\n",
    "    with zipfile.ZipFile(zip_path, mode='w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        zf.write(csv_path, arcname=csv_path.name)\n",
    "    print(\"Created:\", zip_path)\n",
    "\n",
    "sub = OUT/'submission.csv'\n",
    "if sub.exists():\n",
    "    zip_submission(sub, WORK_DIR/'submission.zip')\n",
    "else:\n",
    "    print(\"No submission.csv found — skip zipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b8513",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- **No internet**: dependencies must come from the Kaggle base image or attached datasets.\n",
    "- **Reproducibility**: minimal config snapshot is written to `outputs/config_snapshot.json`.\n",
    "- **Mounting data**: use symlinks inside `/kaggle/working` (see optional cell above) to map Kaggle inputs to your repo layout.\n",
    "- **Schema**: ensure your submission matches the challenge schema (283 μ, 283 σ bins; plus `id`).\n",
    "- **Runtime**: keep per-cell runtimes modest; prefer tested library code + DVC for heavy lifting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

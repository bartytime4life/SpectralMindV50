{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10 \u2014 Alignment & Binning (FGS1 \u2194 AIRS)\n",
        "\n",
        "**Goal:** robust, physics-aware alignment & binning utilities to convert raw/calibrated FGS1 & AIRS into model-ready tensors.\n",
        "\n",
        "**Covers**\n",
        "- Env detection & path resolution (Kaggle vs local repo)\n",
        "- Fast input inventory\n",
        "- Time alignment & **phase folding** helpers\n",
        "- **Binning strategies** (fixed time bins, adaptive bins)\n",
        "- Lightweight **jitter/centroid** decorrelation\n",
        "- Spectral bin integrity checks (expect **283** \u03bc/\u03c3 bins)\n",
        "- Exports compact artifacts under `outputs/`\n",
        "\n",
        "> Keep this light; heavy lifting belongs in library code / DVC stages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83e\udded Environment Detection & Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, platform, glob, math, json\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display  # for rich previews\n",
        "\n",
        "# Headless-safe plotting (CI/Kaggle)\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "BIN_COUNT = 283\n",
        "COMP_DIR = Path('/kaggle/input/ariel-data-challenge-2025')\n",
        "REPO_ROOT_CANDIDATES = [Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent]\n",
        "\n",
        "def detect_env() -> Dict:\n",
        "    env = {\n",
        "        \"is_kaggle\": COMP_DIR.exists(),\n",
        "        \"platform\": platform.platform(),\n",
        "        \"python\": sys.version.replace(\"\\n\", \" \"),\n",
        "        \"cwd\": str(Path.cwd()),\n",
        "        \"repo_root\": None,\n",
        "    }\n",
        "    for c in REPO_ROOT_CANDIDATES:\n",
        "        if (c/'configs').exists() and (c/'schemas').exists():\n",
        "            env[\"repo_root\"] = str(c.resolve())\n",
        "            break\n",
        "    return env\n",
        "\n",
        "ENV = detect_env()\n",
        "ENV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def resolve_paths(env: Dict) -> Dict[str, Optional[Path]]:\n",
        "    repo_root = Path(env['repo_root']) if env['repo_root'] else None\n",
        "    out = {\n",
        "        \"competition\": COMP_DIR if env['is_kaggle'] else None,\n",
        "        \"repo_root\": repo_root,\n",
        "        \"data_raw\": (repo_root/'data'/'raw') if repo_root else None,\n",
        "        \"data_interim\": (repo_root/'data'/'interim') if repo_root else None,\n",
        "        \"data_processed\": (repo_root/'data'/'processed') if repo_root else None,\n",
        "        \"outputs\": Path('outputs'),\n",
        "    }\n",
        "    out[\"outputs\"].mkdir(parents=True, exist_ok=True)\n",
        "    return out\n",
        "\n",
        "PATHS = resolve_paths(ENV)\n",
        "PATHS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udce6 Inventory (Fast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_files(base: Optional[Path], patterns=('*.csv','*.parquet','*.npy','*.npz')):\n",
        "    if not base or not base.exists(): return []\n",
        "    result = []\n",
        "    for pat in patterns:\n",
        "        result.extend([str(p) for p in base.rglob(pat)])\n",
        "    return sorted(result)[:80]\n",
        "\n",
        "inventory = {\n",
        "    \"kaggle_input\": list_files(PATHS[\"competition\"]),\n",
        "    \"data_raw\": list_files(PATHS[\"data_raw\"]),\n",
        "    \"data_interim\": list_files(PATHS[\"data_interim\"]),\n",
        "}\n",
        "inventory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83e\uddee Alignment & Phase Folding Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "@dataclass\n",
        "class Ephemeris:\n",
        "    period: float\n",
        "    t0: float\n",
        "    duration: Optional[float] = None\n",
        "\n",
        "def phase_fold(t: np.ndarray, ephem: Ephemeris) -> np.ndarray:\n",
        "    if ephem.period <= 0: raise ValueError(\"period must be positive\")\n",
        "    phase = ((t - ephem.t0) / ephem.period) % 1.0\n",
        "    phase = np.where(phase >= 0.5, phase - 1.0, phase)\n",
        "    return phase\n",
        "\n",
        "def time_bin(x: np.ndarray, y: np.ndarray, bins: int = 100) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    if len(x) != len(y) or len(x) == 0:\n",
        "        return np.array([]), np.array([]), np.array([])\n",
        "    order = np.argsort(x); x, y = x[order], y[order]\n",
        "    # Robust edges: ensure strictly increasing by epsilon padding\n",
        "    xmin, xmax = float(np.min(x)), float(np.max(x))\n",
        "    if not np.isfinite(xmin) or not np.isfinite(xmax) or xmax <= xmin:\n",
        "        return np.array([]), np.array([]), np.array([])\n",
        "    edges = np.linspace(xmin, xmax, bins + 1)\n",
        "    idx = np.clip(np.digitize(x, edges) - 1, 0, bins-1)\n",
        "    xcs, ym, ys = [], [], []\n",
        "    for b in range(bins):\n",
        "        m = idx == b\n",
        "        if not m.any():\n",
        "            continue\n",
        "        xb = x[m]\n",
        "        yb = y[m]\n",
        "        xcs.append(float(xb.mean()))\n",
        "        ym.append(float(yb.mean()))\n",
        "        ys.append(float(yb.std(ddof=1)) if yb.size > 1 else 0.0)\n",
        "    return np.asarray(xcs), np.asarray(ym), np.asarray(ys)\n",
        "\n",
        "def adaptive_bin(x: np.ndarray, y: np.ndarray, target_count: int = 200) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    if len(x) != len(y) or len(x) == 0:\n",
        "        return np.array([]), np.array([])\n",
        "    order = np.argsort(x); x, y = x[order], y[order]\n",
        "    n = len(x)\n",
        "    # Choose step so ~target_count centers are produced\n",
        "    step = max(1, int(round(n / max(1, target_count))))\n",
        "    centers, means = [], []\n",
        "    for i in range(0, n, step):\n",
        "        j = min(n, i + step)\n",
        "        centers.append(float(x[i:j].mean()))\n",
        "        means.append(float(y[i:j].mean()))\n",
        "    return np.asarray(centers), np.asarray(means)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83e\uddf7 Jitter / Centroid Decorrelation (FGS1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decorrelate_jitter(flux: np.ndarray, cx: Optional[np.ndarray], cy: Optional[np.ndarray]) -> np.ndarray:\n",
        "    \"\"\"Return flux with linear jitter terms removed: flux ~ a + b*cx + c*cy.\"\"\"\n",
        "    if cx is None or cy is None or len(flux) == 0 or len(flux) != len(cx) or len(cx) != len(cy):\n",
        "        return flux\n",
        "    X = np.vstack([np.ones_like(cx), cx, cy]).T\n",
        "    try:\n",
        "        beta, *_ = np.linalg.lstsq(X, flux, rcond=None)\n",
        "        model = X @ beta\n",
        "        return flux - (model - np.mean(model))  # keep original mean\n",
        "    except Exception:\n",
        "        return flux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcd0 Demo (Guarded) with Synthetic or Detected Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "\n",
        "def synth_lightcurve(n=5000, period=1.7, t0=0.3, depth=0.002, noise=5e-4):\n",
        "    t = np.sort(rng.uniform(0, 10*period, size=n))\n",
        "    phase = ((t - t0) / period) % 1.0\n",
        "    transit = (np.abs(phase - 0.5) < 0.02)\n",
        "    flux = 1.0 - depth*transit + rng.normal(0, noise, size=n)\n",
        "    cx = rng.normal(0, 0.05, size=n); cy = rng.normal(0, 0.05, size=n)\n",
        "    flux += 1e-3*cx - 8e-4*cy\n",
        "    return t, flux, cx, cy, Ephemeris(period=period, t0=t0, duration=0.04)\n",
        "\n",
        "loaded = False\n",
        "t, flux, cx, cy, eph = None, None, None, None, None\n",
        "\n",
        "# Heuristic CSV scan (Kaggle)\n",
        "if PATHS[\"competition\"]:\n",
        "    for c in [\"train.csv\", \"test.csv\"]:\n",
        "        p = PATHS[\"competition\"]/c\n",
        "        if p.exists():\n",
        "            try:\n",
        "                df = pd.read_csv(p, nrows=100000)\n",
        "                cand_t = [col for col in df.columns if 'time' in col.lower() or col.lower().startswith('t_')]\n",
        "                cand_f = [col for col in df.columns if 'flux' in col.lower() or 'fgs' in col.lower()]\n",
        "                if cand_t and cand_f:\n",
        "                    t = df[cand_t[0]].to_numpy()\n",
        "                    flux = df[cand_f[0]].to_numpy()\n",
        "                    cx = np.zeros_like(t); cy = np.zeros_like(t)   # placeholder if no centroid\n",
        "                    eph = Ephemeris(period=1.0, t0=float(np.min(t)))\n",
        "                    loaded = True\n",
        "                    break\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "if not loaded:\n",
        "    t, flux, cx, cy, eph = synth_lightcurve()\n",
        "\n",
        "# Decorrelate \u2192 phase fold \u2192 bin\n",
        "flux_dc = decorrelate_jitter(flux, cx, cy)\n",
        "phase = phase_fold(t, eph)\n",
        "xb, yb, yerr = time_bin(t, flux_dc, bins=120)\n",
        "pb, pfm = adaptive_bin(phase, flux_dc, target_count=200)\n",
        "\n",
        "# Plots (pure matplotlib)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(t[:2000], flux_dc[:2000], '.', ms=2)\n",
        "plt.xlabel('time'); plt.ylabel('flux (decorrelated)'); plt.title('Segment (time domain)')\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "if len(xb):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.errorbar(xb, yb, yerr=yerr, fmt='.', ms=4)\n",
        "    plt.xlabel('time'); plt.ylabel('binned flux'); plt.title('Binned time flux')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(phase, flux_dc, '.', ms=2, alpha=0.5)\n",
        "if len(pb): plt.plot(pb, pfm, '-', lw=1)\n",
        "plt.xlabel('phase'); plt.ylabel('flux'); plt.title('Phase folded (adaptive mean)')\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd2c Spectral Bin Integrity (AIRS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "issues = []\n",
        "\n",
        "def count_bin_columns(df: pd.DataFrame, prefix: str) -> int:\n",
        "    return len([c for c in df.columns if c.startswith(prefix)])\n",
        "\n",
        "if PATHS[\"competition\"] and (PATHS[\"competition\"]/'train.csv').exists():\n",
        "    try:\n",
        "        d = pd.read_csv(PATHS[\"competition\"]/'train.csv', nrows=1000)\n",
        "        mu_n = count_bin_columns(d, 'mu_')\n",
        "        sigma_n = count_bin_columns(d, 'sigma_')\n",
        "        if mu_n and mu_n != BIN_COUNT: issues.append(f'Expected {BIN_COUNT} mu_* cols, found {mu_n}')\n",
        "        if sigma_n and sigma_n != BIN_COUNT: issues.append(f'Expected {BIN_COUNT} sigma_* cols, found {sigma_n}')\n",
        "    except Exception as e:\n",
        "        issues.append(f'Failed to read train.csv: {e}')\n",
        "\n",
        "issues or \"Spectral bin shape checks passed (or skipped).\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcbe Export Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = {\n",
        "    \"env\": ENV,\n",
        "    \"n_samples\": int(len(t)),\n",
        "    \"phase_bins\": int(len(pb)),\n",
        "    \"time_bins\": int(len(set(np.digitize(t, np.linspace(t.min(), t.max(), 121))))) if len(t) else 0,\n",
        "    \"issues\": issues,\n",
        "    \"ephemeris\": {\"period\": float(eph.period), \"t0\": float(eph.t0), \"duration\": float(eph.duration) if eph.duration is not None else None} if eph else None,\n",
        "}\n",
        "Path('outputs').mkdir(exist_ok=True, parents=True)\n",
        "with open('outputs/alignment_summary.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "np.savez('outputs/phase_binned.npz', phase_centers=pb, phase_means=pfm)\n",
        "print('Wrote outputs/alignment_summary.json and outputs/phase_binned.npz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "- Hook centroid extraction from calibrated FGS1 cubes; extend decorrelation to higher-order terms  \n",
        "- Replace synthetic demo with competition-specific loaders once columns confirmed  \n",
        "- Parameterize ephemerides from metadata (period, t0) and feed into Hydra configs  \n",
        "- Move helpers into `src/spectramind/pipeline/calibrate.py` and add unit tests"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
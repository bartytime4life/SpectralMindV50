{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1690b8c8",
   "metadata": {},
   "source": [
    "# SpectraMind V50 — Kaggle Prediction Notebook (FGS1 + AIRS)\n\n",
    "**Goal:** Load a trained checkpoint, run inference on Kaggle test set, and write `outputs/submission.csv`.\n\n",
    "**Behavior:** If `spectramind.cli_hooks.notebook_predict` exists, we use it; else, a demo fallback emits a valid zeroed submission with the correct columns.\n\n",
    "**Outputs:**\n",
    "- `outputs/submission.csv`\n",
    "- `outputs/predict_manifest.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, platform, time, warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "BIN_COUNT = 283\n",
    "IS_KAGGLE = Path('/kaggle/input').exists()\n",
    "COMP_DIR = Path('/kaggle/input/ariel-data-challenge-2025') if IS_KAGGLE else Path('./data/kaggle-mock')\n",
    "REPO_ROOT_CANDIDATES = [Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent]\n",
    "\n",
    "def detect_env() -> Dict[str, str]:\n",
    "    env = {\n",
    "        'is_kaggle': IS_KAGGLE,\n",
    "        'platform': platform.platform(),\n",
    "        'python': sys.version.replace('\\n', ' '),\n",
    "        'cwd': str(Path.cwd()),\n",
    "        'repo_root': None,\n",
    "    }\n",
    "    for c in REPO_ROOT_CANDIDATES:\n",
    "        if (c/'configs').exists() and (c/'schemas').exists():\n",
    "            env['repo_root'] = str(c.resolve()); break\n",
    "    return env\n",
    "\n",
    "ENV = detect_env(); ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b806479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_paths(env: Dict) -> Dict[str, Optional[Path]]:\n",
    "    repo_root = Path(env['repo_root']) if env['repo_root'] else None\n",
    "    outputs = Path('outputs'); outputs.mkdir(parents=True, exist_ok=True)\n",
    "    artifacts = Path('artifacts'); artifacts.mkdir(parents=True, exist_ok=True)\n",
    "    return {\n",
    "        'competition': COMP_DIR if env['is_kaggle'] else None,\n",
    "        'repo_root': repo_root,\n",
    "        'configs': (repo_root/'configs') if repo_root else None,\n",
    "        'schemas': (repo_root/'schemas') if repo_root else None,\n",
    "        'artifacts': artifacts,\n",
    "        'outputs': outputs,\n",
    "    }\n",
    "PATHS = resolve_paths(ENV); PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(base: Optional[Path], patterns=('*.csv','*.parquet','*.npz','*.pt','*.pth','*.ckpt'), limit=80):\n",
    "    if not base or not base.exists(): return []\n",
    "    out = []\n",
    "    for pat in patterns:\n",
    "        out.extend([str(p) for p in base.rglob(pat)])\n",
    "    return sorted(out)[:limit]\n",
    "\n",
    "inventory = {\n",
    "    'kaggle_input': list_files(PATHS['competition']) if PATHS['competition'] else [],\n",
    "    'artifacts': list_files(PATHS['artifacts']),\n",
    "}\n",
    "print('Inventory snapshot:')\n",
    "print(json.dumps(inventory, indent=2)[:2000])\n",
    "\n",
    "issues = []\n",
    "if IS_KAGGLE and not (PATHS['competition'] and (PATHS['competition']/ 'test.csv').exists()):\n",
    "    issues.append('Missing test.csv under competition dataset.')\n",
    "if not inventory['artifacts']:\n",
    "    issues.append('No model artifacts found in ./artifacts.')\n",
    "print('\\nIssues:' if issues else '\\nNo blocking issues detected.')\n",
    "for m in issues: print(' -', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9daca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submission_columns() -> Optional[list]:\n",
    "    schema_cols = None\n",
    "    if PATHS['schemas'] and (PATHS['schemas']/ 'submission.schema.json').exists():\n",
    "        try:\n",
    "            with open(PATHS['schemas']/ 'submission.schema.json', 'r', encoding='utf-8') as f:\n",
    "                schema = json.load(f)\n",
    "            fields = schema.get('fields') or schema.get('schema', {}).get('fields')\n",
    "            if isinstance(fields, list) and all('name' in x for x in fields):\n",
    "                schema_cols = [x['name'] for x in fields]\n",
    "        except Exception as e:\n",
    "            print('Failed to parse submission.schema.json:', e)\n",
    "    if schema_cols: return schema_cols\n",
    "    if PATHS['competition'] and (PATHS['competition']/ 'sample_submission.csv').exists():\n",
    "        sample = pd.read_csv(PATHS['competition']/ 'sample_submission.csv', nrows=2)\n",
    "        return list(sample.columns)\n",
    "    return None\n",
    "\n",
    "SUBMISSION_COLS = get_submission_columns()\n",
    "print('Resolved submission columns:', (SUBMISSION_COLS[:10] if SUBMISSION_COLS else None), ' ... total:', (len(SUBMISSION_COLS) if SUBMISSION_COLS else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b604d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_manifest(submission_path: Path, extra: dict):\n",
    "    manifest = {\n",
    "        'submission': str(submission_path.resolve()),\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        **extra\n",
    "    }\n",
    "    with open(PATHS['outputs']/ 'predict_manifest.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    print('Wrote outputs/predict_manifest.json')\n",
    "\n",
    "def validate_submission_frame(df: pd.DataFrame):\n",
    "    if SUBMISSION_COLS is not None:\n",
    "        assert list(df.columns) == list(SUBMISSION_COLS), f'Submission columns mismatch. Expected {len(SUBMISSION_COLS)} columns.'\n",
    "    colnames = [c for c in df.columns if isinstance(c, str)]\n",
    "    mu_cols = [c for c in colnames if c.startswith('mu_')]\n",
    "    sig_cols = [c for c in colnames if c.startswith('sigma_')]\n",
    "    if mu_cols and sig_cols:\n",
    "        if not (len(mu_cols) == BIN_COUNT and len(sig_cols) == BIN_COUNT):\n",
    "            print(f'WARNING: Expected {BIN_COUNT} mu_* and {BIN_COUNT} sigma_* cols; got {len(mu_cols)} and {len(sig_cols)}.')\n",
    "\n",
    "def predict_via_hook() -> Optional[Path]:\n",
    "    try:\n",
    "        from spectramind.cli_hooks import notebook_predict\n",
    "    except Exception as e:\n",
    "        print('spectramind hooks NOT available:', e); return None\n",
    "    config = {\n",
    "        'env': 'kaggle' if IS_KAGGLE else 'local',\n",
    "        'data': {\n",
    "            'competition_dir': str(PATHS['competition']) if PATHS['competition'] else None,\n",
    "            'test_csv': str((PATHS['competition']/ 'test.csv')) if PATHS['competition'] and (PATHS['competition']/ 'test.csv').exists() else None,\n",
    "        },\n",
    "        'inference': {\n",
    "            'checkpoint': inventory['artifacts'][0] if inventory['artifacts'] else None,\n",
    "            'batch_size': 64,\n",
    "            'num_workers': 2,\n",
    "        },\n",
    "        'outputs': {\n",
    "            'dir': str(PATHS['outputs']),\n",
    "            'submission_filename': 'submission.csv',\n",
    "        },\n",
    "        'bins': BIN_COUNT,\n",
    "        'schema_cols': SUBMISSION_COLS,\n",
    "    }\n",
    "    print('Config snapshot (predict):'); print(json.dumps(config, indent=2))\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        submission_path, metrics = notebook_predict(config=config)\n",
    "    except Exception as e:\n",
    "        print('Hooked prediction failed:', e); return None\n",
    "    dt = time.time() - t0\n",
    "    print(f'Hooked prediction completed in {dt:.1f}s')\n",
    "    submission_path = Path(submission_path)\n",
    "    df = pd.read_csv(submission_path)\n",
    "    validate_submission_frame(df)\n",
    "    final_path = PATHS['outputs']/ 'submission.csv'\n",
    "    df.to_csv(final_path, index=False)\n",
    "    write_manifest(final_path, {'mode': 'hook', 'checkpoint': config['inference']['checkpoint'], 'metrics': metrics})\n",
    "    return final_path\n",
    "\n",
    "def predict_demo_fallback() -> Path:\n",
    "    print('Running demo fallback: using sample_submission.csv columns, filling zeros.')\n",
    "    if PATHS['competition'] and (PATHS['competition']/ 'test.csv').exists():\n",
    "        test_df = pd.read_csv(PATHS['competition']/ 'test.csv'); n = len(test_df)\n",
    "    else:\n",
    "        n = 100\n",
    "    if SUBMISSION_COLS is None:\n",
    "        cols = ['id'] + [f'mu_{i:03d}' for i in range(BIN_COUNT)] + [f'sigma_{i:03d}' for i in range(BIN_COUNT)]\n",
    "    else:\n",
    "        cols = SUBMISSION_COLS\n",
    "    sub = pd.DataFrame(0.0, index=np.arange(n), columns=cols)\n",
    "    if PATHS['competition'] and (PATHS['competition']/ 'test.csv').exists():\n",
    "        if 'id' in cols and 'id' in test_df.columns:\n",
    "            sub['id'] = test_df['id'].values[:n]\n",
    "    validate_submission_frame(sub)\n",
    "    final_path = PATHS['outputs']/ 'submission.csv'\n",
    "    sub.to_csv(final_path, index=False)\n",
    "    write_manifest(final_path, {'mode': 'demo', 'metrics': {}})\n",
    "    print('Wrote outputs/submission.csv')\n",
    "    return final_path\n",
    "\n",
    "final = predict_via_hook()\n",
    "if final is None:\n",
    "    final = predict_demo_fallback()\n",
    "print('\\n✔️ Prediction complete. Submission at:', final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56125f04",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Dual encoders + cross-attention used at train time (ADR 0004). Bins = 283 enforced via shape checks.\n",
    "- Submission columns are resolved from schema or `sample_submission.csv` and preserved.\n",
    "- Zero-internet: relies on attached competition dataset + artifact dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

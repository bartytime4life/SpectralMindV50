{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpectraMind V50 — Kaggle **Prediction** Notebook\n",
    "\n",
    "**Purpose:** run inference on the Ariel 2025 competition test set and emit a valid `submission.csv` (and `submission.zip`).  \n",
    "Attach the competition dataset and your SpectraMind V50 code dataset (and optionally a separate **artifacts** dataset containing a trained checkpoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Environment & Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, platform, shutil, time, re, zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "IS_KAGGLE = Path('/kaggle/input').exists()\n",
    "COMP_DIR = Path('/kaggle/input/ariel-data-challenge-2025') if IS_KAGGLE else Path('./data/kaggle-mock')\n",
    "CODE_DS  = Path('/kaggle/input/spectramind-v50') if IS_KAGGLE else Path('./')  # attached code dataset\n",
    "ART_DS   = None  # optional dataset with trained checkpoints (set below if discovered)\n",
    "\n",
    "# Try to detect a likely artifacts dataset under /kaggle/input\n",
    "if IS_KAGGLE:\n",
    "    for p in Path('/kaggle/input').glob('*'):\n",
    "        if (p/'artifacts').exists() and p.name not in {'ariel-data-challenge-2025','spectramind-v50'}:\n",
    "            ART_DS = p\n",
    "            break\n",
    "\n",
    "print(\"Env:\", \"Kaggle\" if IS_KAGGLE else \"Local\", \"| Python:\", sys.version.split()[0])\n",
    "print(\"Competition data:\", COMP_DIR.exists(), str(COMP_DIR))\n",
    "print(\"Code dataset:\", CODE_DS if CODE_DS.exists() else \"not attached\")\n",
    "print(\"Artifacts dataset:\", ART_DS if ART_DS else \"not attached\")\n",
    "\n",
    "# Create output dirs\n",
    "OUT = Path('outputs'); OUT.mkdir(parents=True, exist_ok=True)\n",
    "ART = Path('artifacts'); ART.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add src to path if attached\n",
    "if (CODE_DS/'src').exists():\n",
    "    sys.path.insert(0, str(CODE_DS/'src'))\n",
    "    print(\"Added code src path:\", CODE_DS/'src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Locate Trained Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_checkpoints(*roots):\n",
    "    exts = ('.ckpt', '.pth', '.pt', '.bin', '.sav')\n",
    "    hits = []\n",
    "    for root in roots:\n",
    "        if not root:\n",
    "            continue\n",
    "        try:\n",
    "            for ext in exts:\n",
    "                hits += list(Path(root).rglob(f'*{ext}'))\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Deduplicate and sort by recency / name\n",
    "    hits = sorted(set(hits), key=lambda p: (p.stat().st_mtime if p.exists() else 0, str(p)), reverse=True)\n",
    "    return hits\n",
    "\n",
    "# Candidates: artifacts in this runtime, artifacts inside code dataset, artifacts dataset if any\n",
    "candidates = find_checkpoints(ART, CODE_DS/'artifacts', ART_DS/'artifacts' if ART_DS else None, CODE_DS)\n",
    "for i,p in enumerate(candidates[:5], 1):\n",
    "    print(f\"[{i}] {p}\")\n",
    "\n",
    "CKPT = str(candidates[0]) if candidates else None\n",
    "print(\"Selected checkpoint:\", CKPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Minimal Config Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"env\": \"kaggle\" if IS_KAGGLE else \"local\",\n",
    "    \"data\": {\n",
    "        \"competition_dir\": str(COMP_DIR),\n",
    "        \"test_csv\": str(COMP_DIR/'test.csv'),\n",
    "        \"test_star_info\": str(COMP_DIR/'test_star_info.csv'),\n",
    "        \"axis_info\": str(COMP_DIR/'axis_info.parquet'),\n",
    "        \"sample_submission\": str(COMP_DIR/'sample_submission.csv')\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"batch_size\": 64,\n",
    "        \"tta\": 0\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"checkpoint\": CKPT,\n",
    "        \"name\": \"v50\"\n",
    "    },\n",
    "    \"submission\": {\n",
    "        \"save_csv\": \"outputs/submission.csv\",\n",
    "        \"zip_name\": \"submission.zip\"\n",
    "    }\n",
    "}\n",
    "with open(OUT/'predict_config_snapshot.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(\"Wrote\", OUT/'predict_config_snapshot.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Import SpectraMind predict hook (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from spectramind.cli_hooks import notebook_predict  # should return a pandas DataFrame\n",
    "    HAVE_SM = True\n",
    "    print(\"SpectraMind predict hook available.\")\n",
    "except Exception as e:\n",
    "    HAVE_SM = False\n",
    "    print(\"SpectraMind predict hook NOT available:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Run Inference → `submission.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def save_submission(df: pd.DataFrame, path_csv: Path):\n",
    "    path_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path_csv, index=False)\n",
    "    print(\"Saved submission CSV:\", path_csv)\n",
    "\n",
    "def zip_submission(path_csv: Path, zip_name: str = \"submission.zip\"):\n",
    "    with zipfile.ZipFile(zip_name, mode='w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        zf.write(path_csv, arcname=path_csv.name)\n",
    "    print(\"Created:\", zip_name)\n",
    "\n",
    "sub_df = None\n",
    "\n", 
    "if HAVE_SM and config[\"model\"][\"checkpoint\"]:\n",
    "    # Preferred path: use project hook\n",
    "    sub_df = notebook_predict(config=config, ckpt_path=config[\"model\"][\"checkpoint\"])  # expect DataFrame\n",
    "    assert isinstance(sub_df, pd.DataFrame), \"Predict hook must return a pandas DataFrame\"\n",
    "    print(\"Predict hook returned:\", sub_df.shape)\n",
    "else:\n",
    "    print(\"Falling back to baseline submission scaffold (zeros).\")\n",
    "    ss_path = Path(config[\"data\"][\"sample_submission\"])\n",
    "    if ss_path.exists():\n",
    "        sub_df = pd.read_csv(ss_path)\n",
    "        cols = list(sub_df.columns)\n",
    "        if 'id' not in cols:\n",
    "            test_csv = Path(config[\"data\"][\"test_csv\"])\n",
    "            if test_csv.exists():\n",
    "                tdf = pd.read_csv(test_csv)\n",
    "                sub_df = pd.DataFrame({'id': tdf['id']}) if 'id' in tdf.columns else pd.DataFrame({'id': np.arange(len(tdf))})\n",
    "            else:\n",
    "                sub_df = pd.DataFrame({'id': np.arange(1000)})\n",
    "        # Ensure mu_000..mu_282 and sigma_000..sigma_282 columns exist\n",
    "        for prefix in ('mu_', 'sigma_'):\n",
    "            for i in range(283):\n",
    "                col = f\"{prefix}{i:03d}\"\n",
    "                if col not in sub_df.columns:\n",
    "                    sub_df[col] = 0.0\n",
    "        mu_cols = [f\"mu_{i:03d}\" for i in range(283)]\n",
    "        sg_cols = [f\"sigma_{i:03d}\" for i in range(283)]\n",
    "        sub_df = sub_df[['id'] + mu_cols + sg_cols]\n",
    "    else:\n",
    "        print(\"sample_submission.csv not found; synthesizing minimal frame with 100 ids.\")\n",
    "        sub_df = pd.DataFrame({'id': np.arange(100)})\n",
    "        for prefix in ('mu_', 'sigma_'):\n",
    "            for i in range(283):\n",
    "                sub_df[f\"{prefix}{i:03d}\"] = 0.0\n",
    "\n",
    "csv_path = Path(config[\"submission\"][\"save_csv\"])  # persist\n",
    "save_submission(sub_df, csv_path)\n",
    "zip_submission(csv_path, config[\"submission\"][\"zip_name\"])\n",
    "\n",
    "display(sub_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Register artifacts (manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = {\n",
    "    \"checkpoint_used\": config[\"model\"][\"checkpoint\"],\n",
    "    \"submission_csv\": config[\"submission\"][\"save_csv\"],\n",
    "    \"submission_zip\": config[\"submission\"][\"zip_name\"],\n",
    "    \"rows\": int(sub_df.shape[0]) if sub_df is not None else 0,\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "with open(OUT/'predict_manifest.json', 'w') as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print(\"Wrote\", OUT/'predict_manifest.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Ensure the competition dataset is **attached** so files are available under `/kaggle/input/ariel-data-challenge-2025/` at run and submit time.\n",
    "- Prefer using the `notebook_predict` hook to ensure your pipeline’s preprocessing and model logic are reused consistently.\n",
    "- Keep inference lightweight (no training here). If you need ensembles or TTA, implement them inside the hook and ensure runtime stays within limits.\n",
    "- Validate the shape/schema of the final submission with your Submission Checker notebook if needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

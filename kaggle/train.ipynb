# Create a Kaggle-ready training notebook at /mnt/data/notebooks/kaggle/train.ipynb
# The notebook is conservative (no internet), detects Kaggle vs local, and
# calls into the spectramind package hooks if available.
import os, json, textwrap, nbformat as nbf
from pathlib import Path

nb_path = "/mnt/data/notebooks/kaggle/train.ipynb"
Path(nb_path).parent.mkdir(parents=True, exist_ok=True)

md = lambda s: nbf.v4.new_markdown_cell(textwrap.dedent(s).strip())
code = lambda s: nbf.v4.new_code_cell(textwrap.dedent(s).strip())

nb = nbf.v4.new_notebook()
nb["metadata"].update({
    "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
    "language_info": {"name": "python", "version": "3.x"}
})

nb.cells = [
md("""
# SpectraMind V50 — Kaggle **Training** Notebook

**Purpose:** train the SpectraMind V50 model inside Kaggle (no internet).  
This notebook expects the Ariel competition dataset and your SpectraMind V50 code dataset to be attached.
"""),

md("## 0) Environment & Inputs"),
code("""
import os, sys, json, platform, shutil, time
from pathlib import Path

IS_KAGGLE = Path('/kaggle/input').exists()
COMP_DIR = Path('/kaggle/input/ariel-data-challenge-2025') if IS_KAGGLE else Path('./data/kaggle-mock')
CODE_DS  = Path('/kaggle/input/spectramind-v50') if IS_KAGGLE else Path('./')  # attached code dataset

print("Env:", "Kaggle" if IS_KAGGLE else "Local", "| Python:", sys.version.split()[0])
print("Competition data:", COMP_DIR.exists(), str(COMP_DIR))

# Make sure outputs dir exists
OUT = Path('outputs'); OUT.mkdir(parents=True, exist_ok=True)
ART = Path('artifacts'); ART.mkdir(parents=True, exist_ok=True)

# Add src to path if code dataset is attached
if IS_KAGGLE and (CODE_DS/'src').exists():
    sys.path.insert(0, str(CODE_DS/'src'))
    print("Added code src path:", CODE_DS/'src')
"""),

md("## 1) Minimal Config Snapshot (Hydra-like)"),
code("""
config = {
    "env": "kaggle" if IS_KAGGLE else "local",
    "data": {
        "competition_dir": str(COMP_DIR),
        "train_csv": str(COMP_DIR/'train.csv'),
        "train_star_info": str(COMP_DIR/'train_star_info.csv'),
        "axis_info": str(COMP_DIR/'axis_info.parquet'),
    },
    "training": {
        "seed": 42,
        "epochs": 5,
        "batch_size": 32,
        "lr": 1e-3,
        "precision": "fp32",
        "save_dir": "artifacts"
    },
    "model": {
        "name": "v50",
        "fgs1_encoder": "mamba_ssm-lite",
        "airs_encoder": "cnn-lite",
        "decoder": "heteroscedastic-head"
    }
}
with open(OUT/'config_snapshot.json', 'w') as f:
    json.dump(config, f, indent=2)
print("Wrote", OUT/'config_snapshot.json')
"""),

md("## 2) Optional: Fast data sanity checks"),
code("""
import pandas as pd

def exists(p): 
    try: 
        return Path(p).exists() 
    except Exception: 
        return False

issues = []
if not exists(config["data"]["train_csv"]):
    issues.append("Missing train.csv")
if not exists(config["data"]["train_star_info"]):
    issues.append("Missing train_star_info.csv (optional)")
print("Issues:", issues if issues else "None")

# Peek at train.csv if present (head only to stay light)
if not issues and Path(config["data"]["train_csv"]).exists():
    df = pd.read_csv(config["data"]["train_csv"], nrows=5)
    print("train.csv head:"); display(df.head(3))
"""),

md("## 3) Import SpectraMind hooks (if available)"),
code("""
try:
    # Training hook expected to return the path to saved checkpoint and metrics
    from spectramind.cli_hooks import notebook_train  # your package hook
    HAVE_SM = True
    print("SpectraMind hooks available.")
except Exception as e:
    HAVE_SM = False
    print("SpectraMind hooks NOT available:", e)
"""),

md("## 4) Train"),
code("""
ckpt_path = None
metrics = {}
if HAVE_SM:
    ckpt_path, metrics = notebook_train(config=config)
    print("Checkpoint:", ckpt_path)
    print("Metrics:", metrics)
else:
    print("Falling back to a tiny demo trainer (placeholder).")
    # --- Demo trainer: simulate a training artifact ---
    import numpy as np, json, time
    time.sleep(1.0)
    ckpt_path = str(ART/'model_v50_demo.ckpt')
    with open(ckpt_path, 'wb') as f:
        f.write(os.urandom(256))  # dummy bytes
    metrics = {"train_loss": 0.123, "val_loss": 0.234}
    with open(OUT/'train_metrics.json', 'w') as f:
        json.dump(metrics, f, indent=2)
    print("Saved demo checkpoint:", ckpt_path)
"""),

md("## 5) Register artifacts"),
code("""
manifest = {
    "checkpoint": ckpt_path,
    "metrics": metrics,
    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
}
with open(OUT/'train_manifest.json', 'w') as f:
    json.dump(manifest, f, indent=2)
print("Wrote", OUT/'train_manifest.json')
"""),

md("## 6) Next steps"),
md("""
- Use the saved **checkpoint** for inference in your submission notebook.
- Keep runs deterministic: control seeds; pin dependencies via `requirements-kaggle.txt`.
- Keep the runtime light (≤ 9h, no internet). Prefer smaller epochs when prototyping.
"""),
]

with open(nb_path, "w", encoding="utf-8") as f:
    nbf.write(nb, f)

nb_path

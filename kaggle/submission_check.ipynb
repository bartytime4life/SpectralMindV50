{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpectraMind V50 — Submission Checker (Ariel 2025)\n",
    "\n",
    "This notebook validates a **Kaggle submission** for the NeurIPS 2025 Ariel Data Challenge.\n",
    "\n",
    "It performs:\n",
    "- **Schema checks**: required columns, count (283 μ, 283 σ), dtypes, missing/NaN, order (optional).\n",
    "- **Value checks**: finiteness, σ ≥ 0, configurable bounds on μ and σ, NaN/nonfinite counts.\n",
    "- **File checks**: basic size, duplicate ids, row count.\n",
    "- **Visual QA**: random spectrum plots with μ±σ, histograms and percentiles.\n",
    "- **Report**: machine-readable summary (CSV/JSON) with pass/fail and metrics.\n",
    "\n",
    "> You can set a path to `submission.csv` or a `.zip` containing it (top-level CSV). Defaults to `artifacts/submission.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Path to CSV or ZIP\n",
    "SUBMISSION_PATH = Path('artifacts/submission.csv')  # change if needed\n",
    "\n",
    "# Expected schema\n",
    "N_BINS = 283\n",
    "ID_COL = 'id'\n",
    "MU_PREFIX = 'mu_'\n",
    "SIGMA_PREFIX = 'sigma_'\n",
    "\n",
    "# Soft bounds (permissive defaults; set None to disable either)\n",
    "MU_ABS_MAX = 1.0           # absolute bound for |mu|\n",
    "SIGMA_ABS_MAX = 1.0        # absolute bound for sigma\n",
    "\n",
    "# Random seed for plots\n",
    "SEED = 42\n",
    "\n",
    "SUBMISSION_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load submission (CSV or ZIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, zipfile, io, json, math, time\n",
    "from pathlib import Path\n",
    "\n",
    "def load_submission(path: Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Not found: {path}')\n",
    "    if path.suffix.lower() == '.zip':\n",
    "        with zipfile.ZipFile(path, 'r') as zf:\n",
    "            names = zf.namelist()\n",
    "            cand = None\n",
    "            # prefer standard naming if present\n",
    "            for nm in names:\n",
    "                nm_l = nm.lower()\n",
    "                if nm_l.endswith('submission.csv'):\n",
    "                    cand = nm; break\n", 
    "            if cand is None:\n",
    "                # fallback: first CSV\n",
    "                for nm in names:\n",
    "                    if nm.lower().endswith('.csv'):\n",
    "                        cand = nm; break\n",
    "            if cand is None:\n",
    "                raise ValueError(f'No CSV file found inside zip ({len(names)} entries).')\n",
    "            with zf.open(cand) as f:\n",
    "                return pd.read_csv(f)\n",
    "    else:\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "df = load_submission(SUBMISSION_PATH)\n",
    "print('Loaded:', df.shape)\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Build expected column list & quick helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_columns(n_bins=283, id_col='id', mu_prefix='mu_', sigma_prefix='sigma_'):\n",
    "    mu_cols = [f\"{mu_prefix}{i:03d}\" for i in range(n_bins)]\n",
    "    sg_cols = [f\"{sigma_prefix}{i:03d}\" for i in range(n_bins)]\n",
    "    return [id_col] + mu_cols + sg_cols\n",
    "\n",
    "exp_cols = expected_columns(N_BINS, ID_COL, MU_PREFIX, SIGMA_PREFIX)\n",
    "print('Expected column count:', len(exp_cols))\n",
    "print('First 6:', exp_cols[:6], '...', 'Last 6:', exp_cols[-6:])\n",
    "\n",
    "def list_by_prefix(cols, prefix):\n",
    "    return [c for c in cols if c.startswith(prefix)]\n",
    "\n",
    "def is_numeric_series(series):\n",
    "    return pd.api.types.is_numeric_dtype(series)\n",
    "\n",
    "def count_nonfinite(series):\n",
    "    s = series.to_numpy()\n",
    "    return int(np.sum(~np.isfinite(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Schema validation (presence, dtypes, ids, order optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_report = {}\n",
    "\n",
    "# Column presence (set equality, order optional)\n",
    "have_cols = list(df.columns)\n",
    "missing = [c for c in exp_cols if c not in df.columns]\n",
    "extra = [c for c in df.columns if c not in exp_cols]\n",
    "\n",
    "schema_report['missing_columns'] = missing\n",
    "schema_report['extra_columns'] = extra\n",
    "schema_report['has_all_required'] = len(missing) == 0\n",
    "schema_report['column_count'] = len(df.columns)\n",
    "schema_report['row_count'] = len(df)\n",
    "\n",
    "# Optional: warn if order differs\n",
    "schema_report['order_matches'] = (have_cols == exp_cols)\n",
    "\n",
    "# dtypes for value columns\n",
    "num_cols = [c for c in exp_cols if c != ID_COL and c in df.columns]\n",
    "non_numeric = [c for c in num_cols if not is_numeric_series(df[c])]\n",
    "schema_report['non_numeric_value_columns'] = non_numeric\n",
    "\n",
    "# id uniqueness/non-null\n",
    "id_null = int(df[ID_COL].isna().sum()) if ID_COL in df.columns else -1\n",
    "id_dups = int(df[ID_COL].duplicated().sum()) if ID_COL in df.columns else -1\n",
    "schema_report['id_null_count'] = id_null\n",
    "schema_report['id_duplicate_count'] = id_dups\n",
    "\n",
    "schema_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Value checks (NaN, finiteness, bounds, σ≥0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_report = {}\n",
    "issues = []\n",
    "\n",
    "mu_cols = list_by_prefix(df.columns, MU_PREFIX)\n",
    "sg_cols = list_by_prefix(df.columns, SIGMA_PREFIX)\n",
    "\n",
    "# NaN / nonfinite totals\n",
    "val_report['mu_nan_count'] = int(df[mu_cols].isna().sum().sum()) if mu_cols else -1\n",
    "val_report['sigma_nan_count'] = int(df[sg_cols].isna().sum().sum()) if sg_cols else -1\n",
    "val_report['mu_nonfinite_count'] = sum(count_nonfinite(df[c]) for c in mu_cols) if mu_cols else -1\n",
    "val_report['sigma_nonfinite_count'] = sum(count_nonfinite(df[c]) for c in sg_cols) if sg_cols else -1\n",
    "\n",
    "# Sigma >= 0\n",
    "if sg_cols:\n",
    "    sigma_neg = int((df[sg_cols] < 0).sum().sum())\n",
    "else:\n",
    "    sigma_neg = -1\n",
    "val_report['sigma_negative_count'] = sigma_neg\n",
    "if sigma_neg > 0:\n",
    "    issues.append(f\"Found {sigma_neg} negative sigma values.\")\n",
    "\n",
    "# Soft bounds\n",
    "def count_out_of_bounds(frame, abs_max):\n",
    "    if abs_max is None or frame is None or frame.empty: return 0\n",
    "    arr = np.abs(frame.to_numpy())\n",
    "    return int((arr > abs_max).sum())\n",
    "\n",
    "val_report['mu_out_of_bounds'] = count_out_of_bounds(df[mu_cols] if mu_cols else None, MU_ABS_MAX)\n",
    "val_report['sigma_out_of_bounds'] = count_out_of_bounds(df[sg_cols] if sg_cols else None, SIGMA_ABS_MAX)\n",
    "\n",
    "val_report['issues'] = issues\n",
    "val_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Aggregates & percentiles (μ / σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = {}\n",
    "if mu_cols and sg_cols:\n",
    "    mu_vals = df[mu_cols].to_numpy().ravel()\n",
    "    sg_vals = df[sg_cols].to_numpy().ravel()\n",
    "    for name, arr in [('mu', mu_vals), ('sigma', sg_vals)]:\n",
    "        clean = arr[np.isfinite(arr)]\n",
    "        if clean.size:\n",
    "            q = np.quantile(clean, [0, .001, .01, .05, .5, .95, .99, .999, 1.0])\n",
    "            agg[f'{name}_min'] = float(q[0])\n",
    "            agg[f'{name}_p001'] = float(q[1])\n",
    "            agg[f'{name}_p01'] = float(q[2])\n",
    "            agg[f'{name}_p05'] = float(q[3])\n",
    "            agg[f'{name}_median'] = float(q[4])\n",
    "            agg[f'{name}_p95'] = float(q[5])\n",
    "            agg[f'{name}_p99'] = float(q[6])\n",
    "            agg[f'{name}_p999'] = float(q[7])\n",
    "            agg[f'{name}_max'] = float(q[8])\n",
    "agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Visual QA (random spectra; histograms)\n",
    "Pure matplotlib for portability; keeps plots light and fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def plot_random_spectra(df, mu_cols, sg_cols, n=3):\n",
    "    if not (mu_cols and sg_cols):\n",
    "        print('Missing expected mu/sigma columns; skip spectrum plots.')\n",
    "        return\n",
    "    xs = np.arange(len(mu_cols))\n",
    "    idxs = random.sample(range(len(df)), min(n, len(df)))\n",
    "    for i in idxs:\n",
    "        row = df.iloc[i]\n",
    "        mu = row[mu_cols].to_numpy(dtype=float)\n",
    "        sg = row[sg_cols].to_numpy(dtype=float)\n",
    "        plt.figure(figsize=(9,3))\n",
    "        plt.plot(xs, mu, label='mu')\n",
    "        plt.fill_between(xs, mu - sg, mu + sg, alpha=0.2, label='mu ± sigma')\n",
    "        title_id = row[ID_COL] if ID_COL in df.columns else i\n",
    "        plt.title(f'id={title_id} (row {i})')\n",
    "        plt.xlabel('bin')\n",
    "        plt.ylabel('transit depth')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_hist(arr, title, bins=100, xlim=None):\n",
    "    if arr is None or not arr.size:\n",
    "        return\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.hist(arr[np.isfinite(arr)], bins=bins, alpha=0.85)\n",
    "    if xlim is not None:\n",
    "        plt.xlim(*xlim)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if mu_cols and sg_cols:\n",
    "    plot_random_spectra(df, mu_cols, sg_cols, n=3)\n",
    "    mu_vals = df[mu_cols].to_numpy().ravel()\n",
    "    sg_vals = df[sg_cols].to_numpy().ravel()\n",
    "    plot_hist(mu_vals, 'Histogram: mu (all bins)',\n",
    "              xlim=(-MU_ABS_MAX, MU_ABS_MAX) if MU_ABS_MAX else None)\n",
    "    plot_hist(sg_vals, 'Histogram: sigma (all bins)',\n",
    "              xlim=(0, SIGMA_ABS_MAX) if SIGMA_ABS_MAX else None)\n",
    "else:\n",
    "    print('Skipping visual QA: mu/sigma columns not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Pass/Fail & Report Export\n",
    "Emits `JSON` + a flat `CSV` in `artifacts/submission_checks/` for CI gating or records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "REPORT_DIR = Path('artifacts/submission_checks')\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def make_pass_fail(schema_report, val_report):\n",
    "    ok = True\n",
    "    msgs = []\n",
    "    # Required schema\n",
    "    if not schema_report.get('has_all_required', False):\n",
    "        ok = False; msgs.append('Missing required columns.')\n",
    "    if schema_report.get('id_null_count', 0) > 0:\n",
    "        ok = False; msgs.append('Null IDs present.')\n",
    "    if schema_report.get('id_duplicate_count', 0) > 0:\n",
    "        ok = False; msgs.append('Duplicate IDs present.')\n",
    "    if schema_report.get('non_numeric_value_columns'):\n",
    "        ok = False; msgs.append('Non-numeric value columns detected.')\n",
    "    # Values\n",
    "    if val_report.get('sigma_negative_count', 0) > 0:\n",
    "        ok = False; msgs.append('Found negative sigma values.')\n",
    "    # Optional soft bounds (do not fail by default)\n",
    "    if MU_ABS_MAX is not None and val_report.get('mu_out_of_bounds', 0) > 0:\n",
    "        msgs.append('mu values exceed soft abs bound.')\n",
    "    if SIGMA_ABS_MAX is not None and val_report.get('sigma_out_of_bounds', 0) > 0:\n",
    "        msgs.append('sigma values exceed soft abs bound.')\n",
    "    return ok, msgs\n",
    "\n",
    "ok, messages = make_pass_fail(schema_report, val_report)\n",
    "\n",
    "summary = {\n",
    "    'timestamp': datetime.utcnow().isoformat() + 'Z',\n",
    "    'submission_path': str(SUBMISSION_PATH),\n",
    "    'row_count': schema_report.get('row_count'),\n",
    "    'column_count': schema_report.get('column_count'),\n",
    "    'pass': ok,\n",
    "    'messages': messages,\n",
    "    'schema_report': schema_report,\n",
    "    'value_report': val_report,\n",
    "    'aggregates': agg\n",
    "}\n",
    "\n",
    "# Save JSON and a flat CSV summary\n",
    "json_path = REPORT_DIR / 'submission_check.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "flat = {\n",
    "    'timestamp': summary['timestamp'],\n",
    "    'path': summary['submission_path'],\n",
    "    'rows': summary['row_count'],\n",
    "    'cols': summary['column_count'],\n",
    "    'pass': summary['pass'],\n",
    "    'missing_cols': len(schema_report.get('missing_columns', [])),\n",
    "    'extra_cols': len(schema_report.get('extra_columns', [])),\n",
    "    'id_null': schema_report.get('id_null_count', -1),\n",
    "    'id_dups': schema_report.get('id_duplicate_count', -1),\n",
    "    'mu_nan': val_report.get('mu_nan_count', -1),\n",
    "    'sigma_nan': val_report.get('sigma_nan_count', -1),\n",
    "    'mu_nonfinite': val_report.get('mu_nonfinite_count', -1),\n",
    "    'sigma_nonfinite': val_report.get('sigma_nonfinite_count', -1),\n",
    "    'sigma_negative': val_report.get('sigma_negative_count', -1),\n",
    "    'mu_oob': val_report.get('mu_out_of_bounds', -1),\n",
    "    'sigma_oob': val_report.get('sigma_out_of_bounds', -1),\n",
    "}\n",
    "\n",
    "pd.DataFrame([flat]).to_csv(REPORT_DIR / 'submission_check.csv', index=False)\n",
    "\n",
    "print('PASS' if ok else 'FAIL', '|', '; '.join(messages) if messages else 'OK')\n",
    "print('Saved report to:', json_path, 'and CSV twin in same folder.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

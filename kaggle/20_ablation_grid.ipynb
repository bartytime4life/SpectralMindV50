# %% [markdown]
# # SpectraMind V50 — Loss Ablation Grid
# Runs base and one-at-a-time ablation profiles, summarizes validation metric,
# and emits a CSV + Markdown table.
#
# Profiles covered:
# - base (no ablation)
# - ablation           (all priors off; GLL only)
# - ablation_smoothness
# - ablation_nonneg
# - ablation_band_coherence
# - ablation_calibration
#
# Notes
# - Uses short runs by default for speed; bump epochs via env if needed.
# - Parses `${paths.artifacts}/metrics.jsonl` from each run (or override via SM_JSONL_PATH).
# - Metric preference: `val/gll` (higher is better). Falls back to `val/loss` (lower is better).

# %%
import os
import sys
import json
import time
import shlex
import subprocess
from pathlib import Path
from datetime import datetime
import pandas as pd

# %% [markdown]
# ## Config — adjust if needed

# %%
# CLI entry (module form keeps env/site-packages resolution simple)
CLI = "python -m spectramind train"

# (Optional) switch to a faster profile for smoke runs / Kaggle CI
# Set to "" to disable. Examples:
#   "/training/fast_ci" or "" (none)
FAST_PROFILE = "/training/fast_ci"

# (Optional) environment overrides for ablation runs
DEFAULT_ENV = {
    # keep runs short; change to taste
    "SM_ABL_EPOCHS": os.environ.get("SM_ABL_EPOCHS", "25"),
    "SM_ABL_VAL_INTERVAL": os.environ.get("SM_ABL_VAL_INTERVAL", "0.5"),
    "SM_ABL_LOG_EVERY": os.environ.get("SM_ABL_LOG_EVERY", "20"),
}

# Where metrics.jsonl lands (from callbacks.JsonlMetricsLogger)
# Will also search fallback locations under outputs/ if not found here.
JSONL_ENV_KEY = "SM_JSONL_PATH"

# Output summary
OUTDIR = Path("outputs/ablation_grid")
OUTDIR.mkdir(parents=True, exist_ok=True)
STAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
CSV_PATH = OUTDIR / f"ablation_results_{STAMP}.csv"

# %% [markdown]
# ## Profiles to run

# %%
# Hydra defaults fragments per run (composed on top of your base train.yaml)
def _defaults_of(label: str) -> str:
    parts = []
    if FAST_PROFILE:
        parts.append(FAST_PROFILE)
    if label != "base":
        parts.append(f"/training/{label}")
    # Compose string like +defaults='[/training/fast_ci,/training/ablation_smoothness]'
    return "+defaults='[" + ",".join(parts) + "]'" if parts else ""

RUNS = [
    {"label": "base",                    "defaults": _defaults_of("base")},
    {"label": "ablation",                "defaults": _defaults_of("ablation")},  # all priors off
    {"label": "ablation_smoothness",     "defaults": _defaults_of("ablation_smoothness")},
    {"label": "ablation_nonneg",         "defaults": _defaults_of("ablation_nonneg")},
    {"label": "ablation_band_coherence", "defaults": _defaults_of("ablation_band_coherence")},
    {"label": "ablation_calibration",    "defaults": _defaults_of("ablation_calibration")},
]

# %% [markdown]
# ## Utilities

# %%
def run_training(label: str, defaults: str, extra_env: dict | None = None) -> subprocess.CompletedProcess:
    cmd = f"{CLI} {defaults}".strip()
    print(f"\n[{label}] ➜ {cmd}")
    env = os.environ.copy()
    env.update(DEFAULT_ENV)
    if extra_env:
        env.update(extra_env)
    # Recommended: ensure deterministic-ish seeds if your code supports it
    env.setdefault("PYTHONUNBUFFERED", "1")
    return subprocess.run(shlex.split(cmd), env=env, check=False, text=True, capture_output=True)

def parse_metrics_jsonl(jsonl_path: Path) -> dict:
    """Parse metrics.jsonl and return the last seen vals for keys of interest."""
    if not jsonl_path.exists():
        return {}
    last = {}
    with jsonl_path.open("r", encoding="utf-8") as f:
        for line in f:
            try:
                rec = json.loads(line)
            except Exception:
                continue
            # Normalize keys
            for k, v in rec.items():
                last[k] = v
    return last

def find_jsonl_candidates() -> list[Path]:
    """Fallback search if SM_JSONL_PATH isn't set: scan recent outputs/* directories."""
    candidates = []
    for base in ["outputs", "outputs/train-ablation", "outputs/training", "outputs/train"]:
        p = Path(base)
        if not p.exists():
            continue
        for j in p.rglob("metrics.jsonl"):
            candidates.append(j)
    # sort by mtime (newest first)
    candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    return candidates

def extract_metric(metrics: dict) -> tuple[float | None, str]:
    """Return (value, name) with preference for val/gll (higher is better); else val/loss (lower is better)."""
    # Try common variants users log
    for key in ["val/gll", "val_gll", "gll/val"]:
        if key in metrics and isinstance(metrics[key], (int, float)):
            return float(metrics[key]), key
    for key in ["val/loss", "val_loss", "loss/val"]:
        if key in metrics and isinstance(metrics[key], (int, float)):
            return float(metrics[key]), key
    return None, ""

# %% [markdown]
# ## Execute runs

# %%
records = []
for run in RUNS:
    label = run["label"]
    defaults = run["defaults"]

    # Let user optionally direct logger via env
    jsonl_path = os.environ.get(JSONL_ENV_KEY)
    extra_env = {}
    if jsonl_path:
        extra_env[JSONL_ENV_KEY] = jsonl_path

    # Launch training
    proc = run_training(label, defaults, extra_env)
    if proc.returncode != 0:
        print(f"[{label}] ⚠️ training returned {proc.returncode}")
        print(proc.stderr[-4000:])
    else:
        print(f"[{label}] ✅ done")

    # Resolve metrics.jsonl
    mpath = Path(jsonl_path) if jsonl_path else None
    metrics = {}
    if mpath and mpath.exists():
        metrics = parse_metrics_jsonl(mpath)
    else:
        # Fallback: search outputs
        found = find_jsonl_candidates()
        if found:
            metrics = parse_metrics_jsonl(found[0])

    val, metric_name = extract_metric(metrics)
    records.append({
        "profile": label,
        "metric_name": metric_name or "n/a",
        "metric_value": val,
        "status": "ok" if proc.returncode == 0 and val is not None else "fail"
    })

# %% [markdown]
# ## Summarize & emit results

# %%
df = pd.DataFrame.from_records(records)
# Determine higher-is-better vs lower-is-better using metric name
is_gll = df["metric_name"].str.contains("gll", case=False, na=False)
direction = "higher_is_better" if is_gll.any() else "lower_is_better"

# Compute Δ vs base (sign by direction)
base_val = df.loc[df["profile"] == "base", "metric_value"].values
base_val = float(base_val[0]) if len(base_val) else None

def delta(row):
    if base_val is None or row["metric_value"] is None:
        return None
    if "gll" in (row["metric_name"] or "").lower():
        # higher is better
        return row["metric_value"] - base_val
    else:
        # lower is better
        return base_val - row["metric_value"]

df["delta_vs_base"] = df.apply(delta, axis=1)

# Order rows: base first, then others
order = ["base", "ablation", "ablation_smoothness", "ablation_nonneg", "ablation_band_coherence", "ablation_calibration"]
df["sortkey"] = df["profile"].apply(lambda x: order.index(x) if x in order else 999)
df = df.sort_values(["sortkey"]).drop(columns=["sortkey"])

# Write CSV
df.to_csv(CSV_PATH, index=False)
print(f"\nSaved: {CSV_PATH}")

# Pretty print as Markdown
def fmt(v):
    if v is None or (isinstance(v, float) and (pd.isna(v) or pd.isnull(v))):
        return "—"
    if isinstance(v, float):
        return f"{v:.6f}"
    return str(v)

print("\n### Ablation Results\n")
print("| profile | metric | value | Δ vs base | status |")
print("|---|---:|---:|---:|---|")
for _, r in df.iterrows():
    print(f"| {r['profile']} | {r['metric_name']} | {fmt(r['metric_value'])} | {fmt(r['delta_vs_base'])} | {r['status']} |")

# %% [markdown]
# ### Tips
# - To **re-enable** an ablated prior in a single run without editing YAML:
#   - `SM_ABL_SMOOTH=0.2 python -m spectramind train +defaults='[/training/ablation_smoothness]'`
# - For **fast smoke** on CI/Kaggle:
#   - use the included `/training/fast_ci` profile (short epochs, small limits).
# - Ensure the JSONL logger is enabled (callbacks bundle includes `JsonlMetricsLogger`).
# - If you log only `val/loss`, interpret Δ so *lower is better* (the table already does this).

Here you go ‚Äî a clean copy-and-paste drop you can paste straight into a Kaggle/Colab cell or a local .py/Jupytext notebook. It‚Äôs self-contained, fast, and writes its outputs under outputs/.

‚∏ª

10 ‚Äî Alignment & Binning (FGS1 ‚Üî AIRS)

Goal: robust, physics-aware alignment & binning utilities to convert raw/calibrated FGS1 & AIRS into model-ready tensors.

Covers
	‚Ä¢	Env detection & path resolution (Kaggle vs local repo)
	‚Ä¢	Fast input inventory
	‚Ä¢	Time alignment & phase folding helpers
	‚Ä¢	Binning strategies (fixed time bins, adaptive bins)
	‚Ä¢	Lightweight jitter/centroid decorrelation
	‚Ä¢	Spectral bin integrity checks (expect 283 Œº/œÉ bins)
	‚Ä¢	Exports compact artifacts under outputs/

Keep this light; heavy lifting belongs in library code / DVC stages.

‚∏ª

üß≠ Environment Detection & Paths

import os, sys, platform, glob, math, json
from pathlib import Path
from typing import Optional, Dict, Tuple

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

BIN_COUNT = 283
COMP_DIR = Path('/kaggle/input/ariel-data-challenge-2025')
REPO_ROOT_CANDIDATES = [Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent]

def detect_env() -> Dict:
    env = {
        "is_kaggle": COMP_DIR.exists(),
        "platform": platform.platform(),
        "python": sys.version.replace("\n", " "),
        "cwd": str(Path.cwd()),
        "repo_root": None,
    }
    for c in REPO_ROOT_CANDIDATES:
        if (c/'configs').exists() and (c/'schemas').exists():
            env["repo_root"] = str(c.resolve())
            break
    return env

ENV = detect_env()

def resolve_paths(env: Dict) -> Dict[str, Optional[Path]]:
    repo_root = Path(env['repo_root']) if env['repo_root'] else None
    out = {
        "competition": COMP_DIR if env['is_kaggle'] else None,
        "repo_root": repo_root,
        "data_raw": (repo_root/'data'/'raw') if repo_root else None,
        "data_interim": (repo_root/'data'/'interim') if repo_root else None,
        "data_processed": (repo_root/'data'/'processed') if repo_root else None,
        "outputs": Path('outputs'),
    }
    out["outputs"].mkdir(parents=True, exist_ok=True)
    return out

PATHS = resolve_paths(ENV)
PATHS


‚∏ª

üì¶ Inventory (Fast)

def list_files(base: Optional[Path], patterns=('*.csv','*.parquet','*.npy','*.npz')):
    if not base or not base.exists(): return []
    result = []
    for pat in patterns:
        result.extend([str(p) for p in base.rglob(pat)])
    return sorted(result)[:80]

inventory = {
    "kaggle_input": list_files(PATHS["competition"]),
    "data_raw": list_files(PATHS["data_raw"]),
    "data_interim": list_files(PATHS["data_interim"]),
}
inventory


‚∏ª

üßÆ Alignment & Phase Folding Helpers

from dataclasses import dataclass
from typing import Optional, Tuple

@dataclass
class Ephemeris:
    period: float
    t0: float
    duration: Optional[float] = None

def phase_fold(t: np.ndarray, ephem: Ephemeris) -> np.ndarray:
    if ephem.period <= 0: raise ValueError("period must be positive")
    phase = ((t - ephem.t0) / ephem.period) % 1.0
    phase[phase >= 0.5] -= 1.0
    return phase

def time_bin(x: np.ndarray, y: np.ndarray, bins: int = 100) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    if len(x) != len(y) or len(x) == 0: return np.array([]), np.array([]), np.array([])
    order = np.argsort(x); x, y = x[order], y[order]
    edges = np.linspace(x.min(), x.max(), bins + 1)
    idx = np.digitize(x, edges) - 1
    xcs, ym, ys = [], [], []
    for b in range(bins):
        m = idx == b
        if not m.any(): continue
        xcs.append(x[m].mean())
        ym.append(y[m].mean())
        ys.append(y[m].std(ddof=1) if m.sum() > 1 else 0.0)
    return np.array(xcs), np.array(ym), np.array(ys)

def adaptive_bin(x: np.ndarray, y: np.ndarray, target_count: int = 200) -> Tuple[np.ndarray, np.ndarray]:
    if len(x) != len(y) or len(x) == 0: return np.array([]), np.array([])
    order = np.argsort(x); x, y = x[order], y[order]
    n = len(x)
    step = max(1, n // max(1, (n // max(1, target_count))))
    centers, means = [], []
    for i in range(0, n, step):
        j = min(n, i + step)
        centers.append(x[i:j].mean())
        means.append(y[i:j].mean())
    return np.array(centers), np.array(means)


‚∏ª

üß∑ Jitter / Centroid Decorrelation (FGS1)

def decorrelate_jitter(flux: np.ndarray, cx: Optional[np.ndarray], cy: Optional[np.ndarray]) -> np.ndarray:
    """Return flux with linear jitter terms removed: flux ~ a + b*cx + c*cy."""
    if cx is None or cy is None or len(flux) != len(cx) or len(cx) != len(cy) or len(flux) == 0:
        return flux
    X = np.vstack([np.ones_like(cx), cx, cy]).T
    try:
        beta, *_ = np.linalg.lstsq(X, flux, rcond=None)
        model = X @ beta
        return flux - (model - np.mean(model))  # keep original mean
    except Exception:
        return flux


‚∏ª

üìê Demo (Guarded) with Synthetic or Detected Data

rng = np.random.default_rng(42)

def synth_lightcurve(n=5000, period=1.7, t0=0.3, depth=0.002, noise=5e-4):
    t = np.sort(rng.uniform(0, 10*period, size=n))
    phase = ((t - t0) / period) % 1.0
    transit = (np.abs(phase - 0.5) < 0.02)
    flux = 1.0 - depth*transit + rng.normal(0, noise, size=n)
    cx = rng.normal(0, 0.05, size=n); cy = rng.normal(0, 0.05, size=n)
    flux += 1e-3*cx - 8e-4*cy
    return t, flux, cx, cy, Ephemeris(period=period, t0=t0, duration=0.04)

loaded = False
t, flux, cx, cy, eph = None, None, None, None, None

# Heuristic CSV scan (Kaggle)
if PATHS["competition"]:
    for c in ["train.csv", "test.csv"]:
        p = PATHS["competition"]/c
        if p.exists():
            try:
                df = pd.read_csv(p, nrows=100000)
                cand_t = [col for col in df.columns if 'time' in col.lower() or col.lower().startswith('t_')]
                cand_f = [col for col in df.columns if 'flux' in col.lower() or 'fgs' in col.lower()]
                if cand_t and cand_f:
                    t = df[cand_t[0]].to_numpy()
                    flux = df[cand_f[0]].to_numpy()
                    cx = np.zeros_like(t); cy = np.zeros_like(t)   # placeholder if no centroid
                    eph = Ephemeris(period=1.0, t0=t.min())
                    loaded = True
                    break
            except Exception:
                pass

if not loaded:
    t, flux, cx, cy, eph = synth_lightcurve()

# Decorrelate ‚Üí phase fold ‚Üí bin
flux_dc = decorrelate_jitter(flux, cx, cy)
phase = phase_fold(t, eph)
xb, yb, yerr = time_bin(t, flux_dc, bins=120)
pb, pfm = adaptive_bin(phase, flux_dc, target_count=200)

# Plots (pure matplotlib)
plt.figure(figsize=(6,4))
plt.plot(t[:2000], flux_dc[:2000], '.', ms=2)
plt.xlabel('time'); plt.ylabel('flux (decorrelated)'); plt.title('Segment (time domain)')
plt.tight_layout(); plt.show()

if len(xb):
    plt.figure(figsize=(6,4))
    plt.errorbar(xb, yb, yerr=yerr, fmt='.', ms=4)
    plt.xlabel('time'); plt.ylabel('binned flux'); plt.title('Binned time flux')
    plt.tight_layout(); plt.show()

plt.figure(figsize=(6,4))
plt.plot(phase, flux_dc, '.', ms=2, alpha=0.5)
if len(pb): plt.plot(pb, pfm, '-', lw=1)
plt.xlabel('phase'); plt.ylabel('flux'); plt.title('Phase folded (adaptive mean)')
plt.tight_layout(); plt.show()


‚∏ª

üî¨ Spectral Bin Integrity (AIRS)

issues = []

def count_bin_columns(df: pd.DataFrame, prefix: str) -> int:
    return len([c for c in df.columns if c.startswith(prefix)])

if PATHS["competition"] and (PATHS["competition"]/'train.csv').exists():
    try:
        d = pd.read_csv(PATHS["competition"]/'train.csv', nrows=1000)
        mu_n = count_bin_columns(d, 'mu_')
        sigma_n = count_bin_columns(d, 'sigma_')
        if mu_n and mu_n != BIN_COUNT: issues.append(f'Expected {BIN_COUNT} mu_* cols, found {mu_n}')
        if sigma_n and sigma_n != BIN_COUNT: issues.append(f'Expected {BIN_COUNT} sigma_* cols, found {sigma_n}')
    except Exception as e:
        issues.append(f'Failed to read train.csv: {e}')

issues or "Spectral bin shape checks passed (or skipped)."


‚∏ª

üíæ Export Artifacts

summary = {
    "env": ENV,
    "n_samples": int(len(t)),
    "phase_bins": int(len(pb)),
    "time_bins": int(len(set(np.digitize(t, np.linspace(t.min(), t.max(), 121))))) if len(t) else 0,
    "issues": issues,
    "ephemeris": {"period": eph.period, "t0": eph.t0, "duration": eph.duration} if eph else None,
}
Path('outputs').mkdir(exist_ok=True, parents=True)
with open('outputs/alignment_summary.json', 'w', encoding='utf-8') as f:
    json.dump(summary, f, indent=2)
np.savez('outputs/phase_binned.npz', phase_centers=pb, phase_means=pfm)
print('Wrote outputs/alignment_summary.json and outputs/phase_binned.npz')


‚∏ª

Next Steps
	‚Ä¢	Hook centroid extraction from calibrated FGS1 cubes; extend decorrelation to higher-order terms
	‚Ä¢	Replace synthetic demo with competition-specific loaders once columns confirmed
	‚Ä¢	Parameterize ephemerides from metadata (period, t0) and feed into Hydra configs
	‚Ä¢	Move helpers into src/spectramind/pipeline/calibrate.py and add unit tests

‚∏ª

How to run

Local (repo root)

jupyter notebook notebooks/10_alignment_and_binning.ipynb
# or with jupytext:
jupytext --to ipynb notebooks/10_alignment_and_binning.py

Kaggle
	‚Ä¢	Create a new Notebook from the competition.
	‚Ä¢	Paste the cells, run. Artifacts appear under outputs/.

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 â€” Error Analysis (Î¼/Ïƒ over 283 bins)\n",
    "\n",
    "**Goal:** quantify and visualize model errors for Ariel predictions (Î¼, Ïƒ across 283 bins).\n",
    "\n",
    "**Focus**\n",
    "- Env detection & path resolution (Kaggle vs local repo)\n",
    "- Robust loaders for **ground truth** and **predictions**\n",
    "- Per-bin residual stats, global metrics (MAE/MSE/RMSE, GLL approximation)\n",
    "- **Uncertainty calibration** checks (coverage, PIT)\n",
    "- Reliability diagrams (Ïƒ vs |error|), residual histograms, per-bin RMSE\n",
    "- Artifact export: CSV/JSON summaries in `outputs/`\n",
    "\n",
    "> Pure matplotlib, zero Internet; fast to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§­ Environment & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, platform, math, glob, json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BIN_COUNT = 283\n",
    "COMP_DIR = Path('/kaggle/input/ariel-data-challenge-2025')\n",
    "REPO_ROOT_CANDIDATES = [Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent]\n",
    "\n",
    "def detect_env() -> Dict:\n",
    "    env = {\n",
    "        \"is_kaggle\": COMP_DIR.exists(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "        \"cwd\": str(Path.cwd()),\n",
    "        \"repo_root\": None,\n",
    "    }\n",
    "    for c in REPO_ROOT_CANDIDATES:\n",
    "        if (c/'configs').exists() and (c/'schemas').exists():\n",
    "            env[\"repo_root\"] = str(c.resolve())\n",
    "            break\n",
    "    return env\n",
    "\n",
    "ENV = detect_env()\n",
    "\n",
    "def resolve_paths(env: Dict) -> Dict[str, Optional[Path]]:\n",
    "    repo_root = Path(env['repo_root']) if env['repo_root'] else None\n",
    "    out = {\n",
    "        \"competition\": COMP_DIR if env['is_kaggle'] else None,\n",
    "        \"repo_root\": repo_root,\n",
    "        \"data_processed\": (repo_root/'data'/'processed') if repo_root else None,\n",
    "        \"artifacts\": (repo_root/'artifacts') if repo_root and (repo_root/'artifacts').exists() else None,\n",
    "        \"outputs\": Path('outputs'),\n",
    "    }\n",
    "    out[\"outputs\"].mkdir(parents=True, exist_ok=True)\n",
    "    return out\n",
    "\n",
    "PATHS = resolve_paths(ENV)\n",
    "ENV, PATHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Load Ground Truth & Predictions\n",
    "Automatically searches common locations; you can also place CSVs in `outputs/` and name them `*pred*.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def find_file(candidates: List[Path]) -> Optional[Path]:\n",
    "    for p in candidates:\n",
    "        if p and p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "gt_candidates: List[Path] = []\n",
    "pred_candidates: List[Path] = []\n",
    "\n",
    "if PATHS[\"competition\"]:\n",
    "    gt_candidates += [PATHS[\"competition\"]/'train.csv']\n",
    "if PATHS[\"data_processed\"]:\n",
    "    gt_candidates += list(PATHS[\"data_processed\"].glob(\"**/train*.csv\"))\n",
    "\n",
    "if PATHS[\"artifacts\"]:\n",
    "    pred_candidates += list(PATHS[\"artifacts\"].glob(\"**/*prediction*.csv\"))\n",
    "    pred_candidates += list(PATHS[\"artifacts\"].glob(\"**/*pred*.csv\"))\n",
    "pred_candidates += list(Path(\"outputs\").glob(\"*pred*.csv\"))\n",
    "pred_candidates += list(Path(\".\").glob(\"*pred*.csv\"))\n",
    "\n",
    "GT_PATH = find_file(gt_candidates)\n",
    "PRED_PATH = find_file(pred_candidates)\n",
    "GT_PATH, PRED_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(path: Optional[Path], nrows: Optional[int] = None) -> Optional[pd.DataFrame]:\n",
    "    if not path or not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        if path.suffix == \".csv\":\n",
    "            return pd.read_csv(path, nrows=nrows)\n",
    "        elif path.suffix == \".parquet\":\n",
    "            import pyarrow.parquet as pq\n",
    "            return pq.read_table(path).to_pandas()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {path}: {e}\")\n",
    "    return None\n",
    "\n",
    "gt = load_df(GT_PATH)\n",
    "pred = load_df(PRED_PATH)\n",
    "print(\"Ground truth shape:\", None if gt is None else gt.shape)\n",
    "print(\"Predictions shape:\", None if pred is None else pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Discovery & Merge\n",
    "We detect Î¼/Ïƒ columns and a key for alignment; predictions may not include Ïƒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_columns(df: pd.DataFrame, kind_prefix: str) -> List[str]:\n",
    "    return [c for c in df.columns if c.startswith(kind_prefix)]\n",
    "\n",
    "def find_key(df: pd.DataFrame) -> Optional[str]:\n",
    "    for k in (\"id\", \"sample_id\", \"object_id\"):\n",
    "        if k in df.columns:\n",
    "            return k\n",
    "    return df.columns[0] if len(df.columns) else None\n",
    "\n",
    "cols = {}\n",
    "if gt is not None:\n",
    "    cols[\"gt_mu\"] = discover_columns(gt, \"mu_\")\n",
    "    cols[\"gt_sigma\"] = discover_columns(gt, \"sigma_\")\n",
    "    cols[\"key_gt\"] = find_key(gt)\n",
    "if pred is not None:\n",
    "    cols[\"pred_mu\"] = discover_columns(pred, \"mu_\")\n",
    "    cols[\"pred_sigma\"] = discover_columns(pred, \"sigma_\")\n",
    "    cols[\"key_pred\"] = find_key(pred)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues: List[str] = []\n",
    "\n",
    "def validate_bin_count(mu_cols: List[str], expected: int = BIN_COUNT) -> None:\n",
    "    if mu_cols and len(mu_cols) != expected:\n",
    "        issues.append(f\"Expected {expected} mu_* columns, found {len(mu_cols)}\")\n",
    "\n",
    "if gt is not None:\n",
    "    validate_bin_count(cols.get(\"gt_mu\", []))\n",
    "if pred is not None:\n",
    "    validate_bin_count(cols.get(\"pred_mu\", []))\n",
    "\n",
    "if gt is not None and pred is not None:\n",
    "    key_gt, key_pred = cols.get(\"key_gt\"), cols.get(\"key_pred\")\n",
    "    if key_gt is None or key_pred is None:\n",
    "        issues.append(\"Could not find join key in GT or PRED.\")\n",
    "        merged = None\n",
    "    else:\n",
    "        merged = pd.merge(gt, pred, left_on=key_gt, right_on=key_pred, how=\"inner\", suffixes=(\"_gt\", \"_pred\"))\n",
    "else:\n",
    "    merged = None\n",
    "\n",
    "(len(issues), issues), (None if merged is None else merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¢ Residuals & Metrics (MAE/MSE/RMSE, approx GLL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals_matrix(df: pd.DataFrame, cols_mu_gt: List[str], cols_mu_pred: List[str]) -> Optional[np.ndarray]:\n",
    "    if df is None or not cols_mu_gt or not cols_mu_pred:\n",
    "        return None\n",
    "    try:\n",
    "        mu_gt = df[cols_mu_gt].to_numpy(dtype=float)\n",
    "        mu_pr = df[cols_mu_pred].to_numpy(dtype=float)\n",
    "        if mu_gt.shape != mu_pr.shape:\n",
    "            return None\n",
    "        return mu_pr - mu_gt\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def global_metrics(res: np.ndarray) -> Dict[str, float]:\n",
    "    r = res.flatten()\n",
    "    return {\n",
    "        \"MAE\": float(np.mean(np.abs(r))),\n",
    "        \"MSE\": float(np.mean(r**2)),\n",
    "        \"RMSE\": float(np.sqrt(np.mean(r**2)))\n",
    "    }\n",
    "\n",
    "def approx_gll(mu_gt: np.ndarray, mu_pr: np.ndarray, sigma: Optional[np.ndarray]) -> float:\n",
    "    res = mu_pr - mu_gt\n",
    "    if sigma is None:\n",
    "        s = res.std(axis=0, ddof=1)\n",
    "        sigma = np.maximum(s[None, :], 1e-6)\n",
    "    else:\n",
    "        sigma = np.maximum(sigma, 1e-6)\n",
    "    nll = 0.5*np.log(2*np.pi*sigma**2) + 0.5*(res**2)/(sigma**2)\n",
    "    return float(np.mean(nll))\n",
    "\n",
    "res = residuals_matrix(merged, cols.get(\"gt_mu\", []), cols.get(\"pred_mu\", []))\n",
    "metrics = {}\n",
    "gll_value = None\n",
    "\n",
    "if res is not None:\n",
    "    metrics = global_metrics(res)\n",
    "    mu_gt = merged[cols[\"gt_mu\"]].to_numpy(dtype=float)\n",
    "    mu_pr = merged[cols[\"pred_mu\"]].to_numpy(dtype=float)\n",
    "    if cols.get(\"pred_sigma\"):\n",
    "        sg = merged[cols[\"pred_sigma\"]].to_numpy(dtype=float)\n",
    "    elif cols.get(\"gt_sigma\"):\n",
    "        sg = merged[cols[\"gt_sigma\"]].to_numpy(dtype=float)\n",
    "    else:\n",
    "        sg = None\n",
    "    gll_value = approx_gll(mu_gt, mu_pr, sg)\n",
    "\n",
    "metrics, gll_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Plots (residuals, per-bin RMSE, reliability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_residuals(res: Optional[np.ndarray], bins: int = 80):\n",
    "    if res is None: return\n",
    "    r = res.flatten()\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(r, bins=bins)\n",
    "    plt.xlabel('residual'); plt.ylabel('count'); plt.title('Residuals (global)')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_perbin_rmse(res: Optional[np.ndarray]):\n",
    "    if res is None: return\n",
    "    rmse = np.sqrt(np.mean(res**2, axis=0))\n",
    "    x = np.arange(len(rmse))\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(x, rmse, lw=1)\n",
    "    plt.xlabel('bin index'); plt.ylabel('RMSE'); plt.title('Per-bin RMSE')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def reliability_curve(abs_res: np.ndarray, sigma: np.ndarray, nbins: int = 20):\n",
    "    order = np.argsort(sigma)\n",
    "    abs_res, sigma = abs_res[order], sigma[order]\n",
    "    edges = np.linspace(sigma.min(), sigma.max(), nbins+1)\n",
    "    idx = np.digitize(sigma, edges) - 1\n",
    "    xs, ys = [], []\n",
    "    for b in range(nbins):\n",
    "        m = idx == b\n",
    "        if m.any():\n",
    "            xs.append(sigma[m].mean()); ys.append(abs_res[m].mean())\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "def plot_reliability(res: Optional[np.ndarray], sigma: Optional[np.ndarray]):\n",
    "    if res is None or sigma is None: return\n",
    "    abs_r = np.abs(res).flatten(); s = sigma.flatten()\n",
    "    if len(s) != len(abs_r): return\n",
    "    x, y = reliability_curve(abs_r, s, nbins=25)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(x, y, '.', ms=4)\n",
    "    plt.xlabel('predicted sigma'); plt.ylabel('mean |residual|'); plt.title('Reliability (|res| vs Ïƒ)')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_hist_residuals(res)\n",
    "plot_perbin_rmse(res)\n",
    "\n",
    "sigma_for_rel = None\n",
    "if merged is not None and cols.get(\"pred_sigma\"):\n",
    "    sigma_for_rel = merged[cols[\"pred_sigma\"]].to_numpy(dtype=float)\n",
    "elif merged is not None and cols.get(\"gt_sigma\"):\n",
    "    sigma_for_rel = merged[cols[\"gt_sigma\"]].to_numpy(dtype=float)\n",
    "\n",
    "plot_reliability(res, sigma_for_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Coverage & PIT (Uncertainty Calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import erf\n",
    "\n",
    "def normal_cdf(z: np.ndarray) -> np.ndarray:\n",
    "    return 0.5*(1.0 + erf(z/np.sqrt(2.0)))\n",
    "\n",
    "def coverage(mu: np.ndarray, sigma: np.ndarray, gt: np.ndarray, k: float) -> float:\n",
    "    z = np.abs(gt - mu) / np.maximum(sigma, 1e-9)\n",
    "    return float(np.mean(z <= k))\n",
    "\n",
    "coverage_stats = {}\n",
    "pit_hist = None\n",
    "\n",
    "if merged is not None and cols.get(\"pred_mu\") and cols.get(\"gt_mu\") and sigma_for_rel is not None:\n",
    "    mu_pr = merged[cols[\"pred_mu\"]].to_numpy(dtype=float)\n",
    "    mu_gt = merged[cols[\"gt_mu\"]].to_numpy(dtype=float)\n",
    "    sg = sigma_for_rel\n",
    "    coverage_stats[\"k=1\"] = coverage(mu_pr, sg, mu_gt, k=1.0)\n",
    "    coverage_stats[\"k=2\"] = coverage(mu_pr, sg, mu_gt, k=2.0)\n",
    "\n",
    "    z = (mu_gt - mu_pr)/np.maximum(sg, 1e-9)\n",
    "    pit = normal_cdf(z).flatten()\n",
    "    pit_hist = np.histogram(pit, bins=20, range=(0,1))[0].astype(float)\n",
    "    pit_hist /= pit_hist.sum()\n",
    "\n",
    "coverage_stats, (None if pit_hist is None else pit_hist[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pit_hist is not None:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.bar(np.linspace(0.025,0.975,20), pit_hist, width=0.045)\n",
    "    plt.xlabel('PIT bin center'); plt.ylabel('frequency'); plt.title('PIT histogram (should be ~uniform)')\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¾ Export Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"env\": ENV,\n",
    "    \"shapes\": {\n",
    "        \"gt\": None if gt is None else list(gt.shape),\n",
    "        \"pred\": None if pred is None else list(pred.shape),\n",
    "        \"merged\": None if merged is None else list(merged.shape),\n",
    "    },\n",
    "    \"metrics\": metrics,\n",
    "    \"gll\": gll_value,\n",
    "    \"coverage\": coverage_stats,\n",
    "    \"issues\": issues,\n",
    "}\n",
    "\n",
    "Path('outputs').mkdir(exist_ok=True, parents=True)\n",
    "with open('outputs/error_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "if res is not None:\n",
    "    rmse = np.sqrt(np.mean(res**2, axis=0))\n",
    "    pd.DataFrame({\"bin\": np.arange(len(rmse)), \"rmse\": rmse}).to_csv(\"outputs/per_bin_rmse.csv\", index=False)\n",
    "\n",
    "print(\"Wrote outputs/error_summary.json and (optionally) outputs/per_bin_rmse.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Harmonize column names with your submission format and training pipeline\n",
    "- Add CRPS or weighted GLL consistent with the challenge metric if needed\n",
    "- Integrate with DVC stage to track metrics/artifacts across runs\n",
    "\n",
    "**Done.** âœ…"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

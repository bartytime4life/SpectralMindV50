{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432429f8",
   "metadata": {},
   "source": [
    "# 00 â€” Exploration Notebook (SpectraMind V50)\n",
    "\n",
    "**Purpose**: Fast, repeatable exploratory data analysis (EDA) for the NeurIPS 2025 Ariel Data Challenge\n",
    "with Kaggle-aware paths and zero-internet, physics-informed guardrails.\n",
    "\n",
    "This notebook is designed to run in three environments:\n",
    "\n",
    "1) **Kaggle** (competition dataset auto-mounted under `/kaggle/input/ariel-data-challenge-2025/`)  \n",
    "2) **Local dev** checkout of the SpectraMind V50 repo (expects `data/` + `configs/` structure)  \n",
    "3) **Bare environment** for quick experimentation (graceful skips if files absent)\n",
    "\n",
    "> Tips\n",
    "> - Keep heavy plots light; this notebook should render under 2â€“3 minutes on Kaggle CPU.\n",
    "> - Prefer **matplotlib** for compatibility; avoid seaborn here.\n",
    "> - Do not hardcode secrets/paths; use detection helpers below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c489a",
   "metadata": {},
   "source": [
    "## ðŸ§­ Session Setup & Environment Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168900fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, platform, json, math, glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting: matplotlib only; no seaborn, no style colors specified\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constants\n",
    "BIN_COUNT = 283\n",
    "COMP_DIR = Path('/kaggle/input/ariel-data-challenge-2025')\n",
    "REPO_ROOT_CANDIDATES = [Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent]\n",
    "\n",
    "def detect_env():\n",
    "    env = {\n",
    "        \"is_kaggle\": COMP_DIR.exists(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "        \"cwd\": str(Path.cwd()),\n",
    "    }\n",
    "    # Try to locate repo root (spectramind-v50)\n",
    "    for c in REPO_ROOT_CANDIDATES:\n",
    "        if (c/'configs').exists() and (c/'schemas').exists():\n",
    "            env[\"repo_root\"] = str(c.resolve())\n",
    "            break\n",
    "    else:\n",
    "        env[\"repo_root\"] = None\n",
    "    return env\n",
    "\n",
    "ENV = detect_env()\n",
    "ENV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678428ef",
   "metadata": {},
   "source": [
    "### âœ… Paths & Inputs\n",
    "The following helper resolves key paths depending on where we are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict\n",
    "\n",
    "def resolve_paths(env: Dict) -> Dict[str, Optional[Path]]:\n",
    "    repo_root = Path(env['repo_root']) if env['repo_root'] else None\n",
    "    paths = {\n",
    "        \"competition\": COMP_DIR if env['is_kaggle'] else None,\n",
    "        \"repo_root\": repo_root,\n",
    "        \"data_raw\": (repo_root/'data'/'raw') if repo_root else None,\n",
    "        \"data_processed\": (repo_root/'data'/'processed') if repo_root else None,\n",
    "        \"schemas\": (repo_root/'schemas') if repo_root else None,\n",
    "        \"configs\": (repo_root/'configs') if repo_root else None,\n",
    "        \"artifacts\": (repo_root/'artifacts') if repo_root and (repo_root/'artifacts').exists() else None,\n",
    "        \"outputs\": Path('outputs'),\n",
    "    }\n",
    "    Path('outputs').mkdir(exist_ok=True, parents=True)\n",
    "    return paths\n",
    "\n",
    "PATHS = resolve_paths(ENV)\n",
    "PATHS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b707f",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Inspect Files (Fast Inventory)\n",
    "This will list known CSV/Parquet files if present in Kaggle or local repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d79d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(base: Optional[Path], patterns=('*.csv','*.parquet','*.json')):\n",
    "    if not base or not base.exists():\n",
    "        return []\n",
    "    out = []\n",
    "    for pat in patterns:\n",
    "        out.extend([str(p) for p in base.rglob(pat)])\n",
    "    return sorted(out)[:50]  # cap for readability\n",
    "\n",
    "inventory = {\n",
    "    \"kaggle_input\": list_files(PATHS[\"competition\"]) if PATHS[\"competition\"] else [],\n",
    "    \"data_raw\": list_files(PATHS[\"data_raw\"]),\n",
    "    \"data_processed\": list_files(PATHS[\"data_processed\"]),\n",
    "}\n",
    "inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18311998",
   "metadata": {},
   "source": [
    "## ðŸ§¾ Optional: Validate Submission / Events Schemas (if available)\n",
    "If the repository `schemas/` directory is present, load and preview schema files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d60da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "schemas_preview = {}\n",
    "if PATHS[\"schemas\"] and PATHS[\"schemas\"].exists():\n",
    "    for name in (\"submission.schema.json\", \"events.schema.json\", \"config_snapshot.schema.json\"):\n",
    "        p = PATHS[\"schemas\"]/name\n",
    "        if p.exists():\n",
    "            try:\n",
    "                with open(p, 'r', encoding='utf-8') as f:\n",
    "                    j = json.load(f)\n",
    "                # Preview only top-level keys for brevity\n",
    "                schemas_preview[name] = {\n",
    "                    \"title\": j.get(\"title\"),\n",
    "                    \"$schema\": j.get(\"$schema\"),\n",
    "                    \"required\": j.get(\"required\"),\n",
    "                    \"additionalProperties\": j.get(\"additionalProperties\"),\n",
    "                }\n",
    "            except Exception as e:\n",
    "                schemas_preview[name] = {\"error\": str(e)}\n",
    "schemas_preview if schemas_preview else \"No local schemas found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a6210",
   "metadata": {},
   "source": [
    "## ðŸ”Ž First Look â€” Tabular Summaries\n",
    "The Kaggle competition typically provides `train.csv`, `test.csv`, and metadata tables.\n",
    "This section attempts to read common filenames if present. All steps are **guarded** to avoid hard failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5e8469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_read_csv(p: Path, **kw):\n",
    "    try:\n",
    "        return pd.read_csv(p, **kw)\n",
    "    except Exception as e:\n",
    "        print(f'Failed to read {p}: {e}')\n",
    "        return None\n",
    "\n",
    "dfs = {}\n",
    "if PATHS[\"competition\"]:\n",
    "    for candidate in (\"train.csv\", \"test.csv\", \"train_star_info.csv\", \"test_star_info.csv\",\n",
    "                      \"adc_info.csv\", \"axis_info.parquet\", \"wavelengths.csv\", \"sample_submission.csv\"):\n",
    "        p = PATHS[\"competition\"]/candidate\n",
    "        if p.exists():\n",
    "            if p.suffix == '.csv':\n",
    "                dfs[candidate] = safe_read_csv(p)\n",
    "            elif p.suffix == '.parquet':\n",
    "                try:\n",
    "                    import pyarrow.parquet as pq\n",
    "                    dfs[candidate] = pq.read_table(p).to_pandas()\n",
    "                except Exception as e:\n",
    "                    print(f'Failed to read parquet {p}: {e}')\n",
    "\n",
    "# Preview heads\n",
    "preview = {k: v.head(5) if isinstance(v, pd.DataFrame) else None for k,v in dfs.items()}\n",
    "list(preview.keys()), {k: v.shape for k,v in dfs.items() if hasattr(v, 'shape')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd1d3dc",
   "metadata": {},
   "source": [
    "### Basic Profiling Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5721736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_df(df: pd.DataFrame, name: str, max_cols: int = 20):\n",
    "    print(f'\\n=== {name} ===')\n",
    "    print(f'shape: {df.shape}')\n",
    "    display(df.head(3))\n",
    "    display(df.describe(include='all').T.head(max_cols))\n",
    "    nulls = df.isnull().mean().sort_values(ascending=False)\n",
    "    display(nulls.head(max_cols).to_frame('null_frac'))\n",
    "\n",
    "for k, df in dfs.items():\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        profile_df(df, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd62a4",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Quick Plots (Matplotlib)\n",
    "These keep colors default and avoid style settings. They will plot only if the expected columns exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba57004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_hist(df: pd.DataFrame, col: str, bins: int = 50, title: str = None):\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        df[col].dropna().plot(kind='hist', bins=bins)\n",
    "        plt.title(title or f'Histogram â€” {col}')\n",
    "        plt.xlabel(col); plt.ylabel('count')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def maybe_line(df: pd.DataFrame, x: str, y: str, n: int = 1000, title: str = None):\n",
    "    if x in df.columns and y in df.columns:\n",
    "        sample = df[[x,y]].dropna().head(n)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(sample[x].values, sample[y].values)\n",
    "        plt.title(title or f'{y} vs {x}')\n",
    "        plt.xlabel(x); plt.ylabel(y)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example heuristics\n",
    "if \"train.csv\" in dfs:\n",
    "    df = dfs[\"train.csv\"]\n",
    "    for candidate in [\"id\", \"target\", \"mu_000\", \"sigma_000\"]:\n",
    "        if candidate in df.columns:\n",
    "            maybe_hist(df, candidate, bins=40, title=f'{candidate} distribution')\n",
    "\n",
    "if \"axis_info.parquet\" in dfs:\n",
    "    df = dfs[\"axis_info.parquet\"]\n",
    "    cols = df.columns.tolist()\n",
    "    if len(cols) >= 2:\n",
    "        maybe_line(df, cols[0], cols[1], title=f'{cols[1]} vs {cols[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34916c37",
   "metadata": {},
   "source": [
    "## ðŸ§ª Sanity Checks (Physics-Informed)\n",
    "A few lightweight checks that won't assume file presence but will validate shape conventions when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f0dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = []\n",
    "\n",
    "# BIN_COUNT expectation\n",
    "if \"train.csv\" in dfs:\n",
    "    df = dfs[\"train.csv\"]\n",
    "    mu_cols = [c for c in df.columns if c.startswith('mu_')]\n",
    "    sigma_cols = [c for c in df.columns if c.startswith('sigma_')]\n",
    "    if mu_cols and len(mu_cols) != BIN_COUNT:\n",
    "        issues.append(f'Expected {BIN_COUNT} mu_* columns, found {len(mu_cols)}')\n",
    "    if sigma_cols and len(sigma_cols) != BIN_COUNT:\n",
    "        issues.append(f'Expected {BIN_COUNT} sigma_* columns, found {len(sigma_cols)}')\n",
    "\n",
    "# Non-negativity sanity on sigma (uncertainties)\n",
    "if \"train.csv\" in dfs and 'sigma_000' in dfs[\"train.csv\"].columns:\n",
    "    neg_frac = (dfs[\"train.csv\"].filter(like='sigma_') < 0).mean().mean()\n",
    "    if pd.notnull(neg_frac) and neg_frac > 0:\n",
    "        issues.append(f'Negative sigma fraction observed: {neg_frac:.4f}')\n",
    "\n",
    "issues or \"All basic sanity checks passed (or skipped due to missing inputs).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb1695",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Export Light Report (CSV/JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f119e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"env\": ENV,\n",
    "    \"file_counts\": {k: len(v) if isinstance(v, list) else 0 for k,v in (inventory.items())},\n",
    "    \"dataframes\": {k: {\"rows\": int(v.shape[0]), \"cols\": int(v.shape[1])} for k,v in dfs.items() if hasattr(v,'shape')},\n",
    "    \"issues\": issues,\n",
    "}\n",
    "\n",
    "pd.DataFrame(summary[\"dataframes\"]).T.to_csv('outputs/df_shapes.csv', index=True)\n",
    "with open('outputs/summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print('Wrote outputs/df_shapes.csv and outputs/summary.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd201f",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Explore FGS1 lightcurve structure and AIRS spectral axes (`axis_info.parquet`) if available.\n",
    "- Wire this notebook to your **Hydra** configs to reproduce exact preprocessing.\n",
    "- Save any useful visualizations under `outputs/` to keep the notebook clean and CI-friendly.\n",
    "- Keep runtime short; prefer sampling large tables.\n",
    "\n",
    "**Done.** ðŸŽ¯"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba611c62",
   "metadata": {},
   "source": [
    "# 10 ‚Äî Alignment & Binning (FGS1 ‚Üî AIRS)\n",
    "\n",
    "**Goal**: robust, physics‚Äëaware alignment and binning utilities to transform raw/Calibrated FGS1 & AIRS data into\n",
    "model‚Äëready tensors. This notebook is Kaggle‚Äëaware, zero‚Äëinternet, and uses only `matplotlib` for plots.\n",
    "\n",
    "**Covers**\n",
    "- Environment detection & path resolution (Kaggle vs local repo)\n",
    "- Quick inventory of candidate inputs\n",
    "- Time‚Äëdomain alignment & **phase folding** helpers\n",
    "- **Binning strategies** (time binning, adaptive binning, fixed spectral bin checks)\n",
    "- Lightweight **jitter/centroid** outline and sanity checks\n",
    "- Exports concise artifacts under `outputs/`\n",
    "\n",
    "> Keep this light (<~3 min) and reproducible; heavy lifting should live in library code and DVC stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefbcac8",
   "metadata": {},
   "source": [
    "## üß≠ Environment Detection & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, platform, glob, math, json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BIN_COUNT = 283\n",
    "COMP_DIR = Path('/kaggle/input/ariel-data-challenge-2025')\n",
    "REPO_ROOT_CANDIDATES = [Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent]\n",
    "\n",
    "def detect_env() -> Dict:\n",
    "    env = {\n",
    "        \"is_kaggle\": COMP_DIR.exists(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "        \"cwd\": str(Path.cwd()),\n",
    "        \"repo_root\": None,\n",
    "    }\n",
    "    for c in REPO_ROOT_CANDIDATES:\n",
    "        if (c/'configs').exists() and (c/'schemas').exists():\n",
    "            env[\"repo_root\"] = str(c.resolve())\n",
    "            break\n",
    "    return env\n",
    "\n",
    "ENV = detect_env()\n",
    "ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab27837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_paths(env: Dict) -> Dict[str, Optional[Path]]:\n",
    "    repo_root = Path(env['repo_root']) if env['repo_root'] else None\n",
    "    out = {\n",
    "        \"competition\": COMP_DIR if env['is_kaggle'] else None,\n",
    "        \"repo_root\": repo_root,\n",
    "        \"data_raw\": (repo_root/'data'/'raw') if repo_root else None,\n",
    "        \"data_interim\": (repo_root/'data'/'interim') if repo_root else None,\n",
    "        \"data_processed\": (repo_root/'data'/'processed') if repo_root else None,\n",
    "        \"outputs\": Path('outputs'),\n",
    "    }\n",
    "    out[\"outputs\"].mkdir(parents=True, exist_ok=True)\n",
    "    return out\n",
    "\n",
    "PATHS = resolve_paths(ENV)\n",
    "PATHS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110075bd",
   "metadata": {},
   "source": [
    "## üì¶ Inventory (Fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e766f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(base: Optional[Path], patterns=('*.csv','*.parquet','*.npy','*.npz')):\n",
    "    if not base or not base.exists():\n",
    "        return []\n",
    "    result = []\n",
    "    for pat in patterns:\n",
    "        result.extend([str(p) for p in base.rglob(pat)])\n",
    "    return sorted(result)[:80]\n",
    "\n",
    "inventory = {\n",
    "    \"kaggle_input\": list_files(PATHS[\"competition\"]),\n",
    "    \"data_raw\": list_files(PATHS[\"data_raw\"]),\n",
    "    \"data_interim\": list_files(PATHS[\"data_interim\"]),\n",
    "}\n",
    "inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdfdee8",
   "metadata": {},
   "source": [
    "## üßÆ Alignment & Phase Folding Helpers\n",
    "\n",
    "These utilities are **guarded** and unit‚Äëfriendly. They do not assume specific file structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Ephemeris:\n",
    "    period: float           # orbital period (same time unit as timestamps)\n",
    "    t0: float               # reference transit epoch\n",
    "    duration: Optional[float] = None  # optional: transit duration (same unit)\n",
    "\n",
    "def phase_fold(t: np.ndarray, ephem: Ephemeris) -> np.ndarray:\n",
    "    \"\"\"Return phase in [-0.5, 0.5) given timestamps t and ephemeris.\"\"\"\n",
    "    if ephem.period <= 0:\n",
    "        raise ValueError(\"period must be positive\")\n",
    "    phase = ((t - ephem.t0) / ephem.period) % 1.0\n",
    "    phase[phase >= 0.5] -= 1.0\n",
    "    return phase\n",
    "\n",
    "def time_bin(x: np.ndarray, y: np.ndarray, bins: int = 100) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Simple fixed-count binning in time; returns (x_bin_centers, y_mean, y_std).\"\"\"\n",
    "    if len(x) != len(y) or len(x) == 0:\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "    order = np.argsort(x)\n",
    "    x, y = x[order], y[order]\n",
    "    edges = np.linspace(x.min(), x.max(), bins + 1)\n",
    "    idx = np.digitize(x, edges) - 1\n",
    "    xcs, ym, ys = [], [], []\n",
    "    for b in range(bins):\n",
    "        m = idx == b\n",
    "        if not m.any():\n",
    "            continue\n",
    "        xcs.append(x[m].mean())\n",
    "        ym.append(y[m].mean())\n",
    "        ys.append(y[m].std(ddof=1) if m.sum() > 1 else 0.0)\n",
    "    return np.array(xcs), np.array(ym), np.array(ys)\n",
    "\n",
    "def adaptive_bin(x: np.ndarray, y: np.ndarray, target_count: int = 200) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Adaptive binning to maintain ~constant samples per bin; returns centers and means.\"\"\"\n",
    "    if len(x) != len(y) or len(x) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    order = np.argsort(x); x, y = x[order], y[order]\n",
    "    n = len(x)\n",
    "    step = max(1, n // max(1, (n // max(1, target_count))))\n",
    "    centers, means = [], []\n",
    "    for i in range(0, n, step):\n",
    "        j = min(n, i + step)\n",
    "        centers.append(x[i:j].mean())\n",
    "        means.append(y[i:j].mean())\n",
    "    return np.array(centers), np.array(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e12e0c",
   "metadata": {},
   "source": [
    "## üß∑ Jitter / Centroid Outline (FGS1)\n",
    "\n",
    "If centroid tracks (x(t), y(t)) are provided (from calibrated cubes), we can regress out common‚Äëmode jitter.\n",
    "Below is a simple linear decorrelation utility; in practice you may want higher‚Äëorder or spline terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447003da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorrelate_jitter(flux: np.ndarray, cx: Optional[np.ndarray], cy: Optional[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Return flux with linear jitter terms removed: flux ~ a + b*cx + c*cy.\"\"\"\n",
    "    if cx is None or cy is None or len(flux) != len(cx) or len(cx) != len(cy) or len(flux) == 0:\n",
    "        return flux\n",
    "    X = np.vstack([np.ones_like(cx), cx, cy]).T\n",
    "    try:\n",
    "        beta, *_ = np.linalg.lstsq(X, flux, rcond=None)\n",
    "        model = X @ beta\n",
    "        return flux - (model - np.mean(model))  # keep original mean scale\n",
    "    except Exception:\n",
    "        return flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c63e0",
   "metadata": {},
   "source": [
    "## üìê Demo (Guarded) with Synthetic or Detected Data\n",
    "\n",
    "We attempt to detect plausible light‚Äëcurve columns. If none found, we synthesize a small example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948cf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def synth_lightcurve(n=5000, period=1.7, t0=0.3, depth=0.002, noise=5e-4):\n",
    "    t = np.sort(rng.uniform(0, 10*period, size=n))\n",
    "    phase = ((t - t0) / period) % 1.0\n",
    "    transit = (np.abs(phase - 0.5) < 0.02)  # crude box\n",
    "    flux = 1.0 - depth*transit + rng.normal(0, noise, size=n)\n",
    "    cx = rng.normal(0, 0.05, size=n); cy = rng.normal(0, 0.05, size=n)\n",
    "    flux += 1e-3*cx - 8e-4*cy  # inject jitter correlation\n",
    "    return t, flux, cx, cy, Ephemeris(period=period, t0=t0, duration=0.04)\n",
    "\n",
    "# Try loading from Kaggle tables\n",
    "loaded = False\n",
    "t, flux, cx, cy, eph = None, None, None, None, None\n",
    "\n",
    "if PATHS[\"competition\"]:\n",
    "    # Heuristics: look for any CSV with columns like time, flux or fgs1_flux\n",
    "    for c in [\"train.csv\", \"test.csv\"]:\n",
    "        p = PATHS[\"competition\"]/c\n",
    "        if p.exists():\n",
    "            try:\n",
    "                df = pd.read_csv(p, nrows=100000)\n",
    "                cand_t = [col for col in df.columns if 'time' in col.lower() or 't_' == col[:2].lower()]\n",
    "                cand_f = [col for col in df.columns if 'flux' in col.lower() or 'fgs' in col.lower()]\n",
    "                if cand_t and cand_f:\n",
    "                    t = df[cand_t[0]].to_numpy()\n",
    "                    flux = df[cand_f[0]].to_numpy()\n",
    "                    cx = df[cand_t[0]].to_numpy()*0  # placeholder if no centroid\n",
    "                    cy = df[cand_t[0]].to_numpy()*0\n",
    "                    eph = Ephemeris(period=1.0, t0=t.min())\n",
    "                    loaded = True\n",
    "                    break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "if not loaded:\n",
    "    t, flux, cx, cy, eph = synth_lightcurve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorrelate jitter (if centroids present)\n",
    "flux_dc = decorrelate_jitter(flux, cx, cy)\n",
    "\n",
    "# Phase fold\n",
    "phase = phase_fold(t, eph)\n",
    "\n",
    "# Bin in time and in phase\n",
    "xb, yb, yerr = time_bin(t, flux_dc, bins=120)\n",
    "pb, pfm = adaptive_bin(phase, flux_dc, target_count=200)\n",
    "\n",
    "# Plots: default colors only\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(t[:2000], flux_dc[:2000], '.', ms=2)\n",
    "plt.xlabel('time'); plt.ylabel('flux (decorrelated)'); plt.title('Segment (time domain)')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "if len(xb):\n",
    "    plt.errorbar(xb, yb, yerr=yerr, fmt='.', ms=4)\n",
    "    plt.xlabel('time'); plt.ylabel('binned flux'); plt.title('Binned time flux')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(phase, flux_dc, '.', ms=2, alpha=0.5)\n",
    "if len(pb):\n",
    "    plt.plot(pb, pfm, '-', lw=1)\n",
    "plt.xlabel('phase'); plt.ylabel('flux'); plt.title('Phase folded (with adaptive mean)')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6806fe",
   "metadata": {},
   "source": [
    "## üî¨ Spectral Bin Integrity (AIRS)\n",
    "\n",
    "Quick check for expected **283** spectral bins `(mu_*, sigma_*)` when table layout provides them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = []\n",
    "\n",
    "def count_bin_columns(df: pd.DataFrame, prefix: str) -> int:\n",
    "    return len([c for c in df.columns if c.startswith(prefix)])\n",
    "\n",
    "if PATHS[\"competition\"] and (PATHS[\"competition\"]/'train.csv').exists():\n",
    "    try:\n",
    "        d = pd.read_csv(PATHS[\"competition\"]/'train.csv', nrows=1000)\n",
    "        mu_n = count_bin_columns(d, 'mu_')\n",
    "        sigma_n = count_bin_columns(d, 'sigma_')\n",
    "        if mu_n and mu_n != BIN_COUNT:\n",
    "            issues.append(f'Expected {BIN_COUNT} mu_* cols, found {mu_n}')\n",
    "        if sigma_n and sigma_n != BIN_COUNT:\n",
    "            issues.append(f'Expected {BIN_COUNT} sigma_* cols, found {sigma_n}')\n",
    "    except Exception as e:\n",
    "        issues.append(f'Failed to read train.csv: {e}')\n",
    "\n",
    "issues or \"Spectral bin shape checks passed (or skipped).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc7d76",
   "metadata": {},
   "source": [
    "## üíæ Export Artifacts\n",
    "\n",
    "We persist a small alignment/phase summary and any detected issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1350122",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"env\": ENV,\n",
    "    \"n_samples\": int(len(t)),\n",
    "    \"phase_bins\": int(len(pb)),\n",
    "    \"time_bins\": int(len(set(np.digitize(t, np.linspace(t.min(), t.max(), 121))))) if len(t) else 0,\n",
    "    \"issues\": issues,\n",
    "    \"ephemeris\": {\"period\": eph.period, \"t0\": eph.t0, \"duration\": eph.duration} if eph else None,\n",
    "}\n",
    "Path('outputs').mkdir(exist_ok=True, parents=True)\n",
    "with open('outputs/alignment_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "np.savez('outputs/phase_binned.npz', phase_centers=pb, phase_means=pfm)\n",
    "print('Wrote outputs/alignment_summary.json and outputs/phase_binned.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce9dca",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Integrate centroid extraction from calibrated FGS1 cubes; extend decorrelation to higher‚Äëorder terms\n",
    "- Replace synthetic demo with competition‚Äëspecific loaders once confirmed\n",
    "- Parameterize ephemeris from metadata tables (period, t0) and propagate to Hydra configs\n",
    "- Push these utilities into `src/spectramind/pipeline/calibrate.py` and cover with unit tests\n",
    "\n",
    "**Done.** ‚úÖ"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
